# Activity

- Now it's your turn!
- You can find a data set titled "clinician.csv" in the supplemental material linked [here](https://github.com/jimmyrigby94/Data-Management-in-R/tree/master/suppl).
- A .txt file (clinician_description.txt) is also included describing the data and structure.
- Your goal is to clean the data and then summarise the data in a meaningful way. 
- Instructions may, at times, be intentionally ambiguous.
- This is intended to facilitate critical thinking when applying the principles learned. 

<div class = activity>
<div class = activitytitle>
Instructions</div>
**Data Cleaning**

1. Create a dataframe with only the current patients.

2. Remove irrelevant columns. 

3. Assign meaningful names to all columns.

4. Remove observatons that have NO data for Beck Depression Inventory and Subjective Well-Being scales.

5. Create scale scores using arithmetic operators.

6. Create scale scores using a method that employs a function to generate the mean.

7. Create a varaible that stores the depression CDF for each participant. 


**Group-Level Feature Extraction**

1. Create a variable that stores therapist-level depression scores for each participant

2. Create a variable that stores therapist-level attrition rate. 

**Summary Statistics**

1. Create a separate data frame that summarises the sample's central tendency (i.e., mean and median) and spread (i.e., variance, standard deviation, and mad). 

2. Create an identical table, except calculate these summary statistics across therapists.

3. Which therapist has the highest attrition rate?

4. Which therapist has the highest recovery rate?

5. Is there an association between the Recrutiment Medium and recovery rate?

**Follow-up Quesitons**

1. What was the difference between scale scores generated by arithmetic operators and functions?

2. Did the information stored within group-level features differ from what was generated using grouped summary statistics?

3. When would it be useful to store group-level data as a summary table versus a variable in the original data frame?
</div>

## Solutions

Below are the solutions for the activity. First I answer each question individually. After that, I use a single dplyr chain to complete the Data Cleaning and Group-level Feature Extraction exercises. For each function call in the chain, I explain what the functions are doing. Note that functions ending in _at(), _all(), or _if are advanced function that save time. The associated base calls are just as appropriate. 

**Data Cleaning**

```{r}
# Preporation work ----------------------
library(tidyverse)

# Read the data
clinician<-read_csv("suppl/clinician.csv")
```

1. Create a dataframe with only the current patients.

```{r}
clean_clinician<-clinician%>%
  filter(attrition!=1) # Removing patients that no longer attend therapy (i.e., attrition == 1)
```



2. Remove irrelevant columns.

```{r}
# Dropping variables with no variance (Pull_Date and
# attrition were the same across each obs)
clean_clinician<-clean_clinician%>%
    select(-Pull_Date, -attrition) 
```


3. Assign meaningful names to all columns.
```{r}
 # renaming to meaningful variables
clean_clinician<-clean_clinician%>%
  rename(beck1 = Q1.1, beck2 = Q1.2, beck3 = Q1.3,
         swb1 = Q2.1, swb2 = Q2.2, swb3 = Q2.3)
```


4. Remove observatons that have NO data for Beck Depression Inventory and Subjective Well-Being scales.
```{r}
# Removing observations if all are missing (retaining if responded on any observations)
clean_clinician<-clean_clinician%>%
   filter(!is.na(beck1)|!is.na(beck2)|!is.na(beck3)|
            !is.na(swb1)|!is.na(swb2)|!is.na(swb3)) 

```

5. Create scale scores using arithmetic operators.
```{r}
 #Creating a new column called beck_arith using arithmetic methods for means
clean_clinician<-clean_clinician%>%
  mutate(beck_arith = (beck1+beck2+beck3)/3, 
         swb_arith = (swb1+swb2+swb3)/3)
```

6. Create scale scores using a method that employs a function to generate the mean.
```{r}
clean_clinician<-clean_clinician%>%
  rowwise()%>% # Creating scale scores with missing data
  mutate(beck_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE), 
         swb_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE))%>% # Using function for means
  ungroup()# always making sure to ungroup

```

7. Create a varaible that stores the depression CDF for each participant. 
```{r}
# creating a new variable that contains the cdf for becks depression inventory
clean_clinician<-clean_clinician%>%
  mutate(dep_cdf = cume_dist(beck_fun)) 

```


**Group-Level Feature Extraction**

1. Create a variable that stores therapist-level depression scores for each participant
```{r}
clean_clinician<-clean_clinician%>% # Using the cleaned data overwrite clean data.
    group_by(Therapist)%>% # group the data by Therapist
    mutate(dep_cdf_grp = mean(beck_fun, na.rm = TRUE))%>% # calculate the cdf by group
    ungroup()
```

2. Create a variable that stores therapist-level attrition rate. 

Attrtion rate is no longer stored in this data. We deselected it above. Below, I show how to use join to add it to the cleaned data. A more efficient way would be to create the attr_count variable before I filtered out former patients. (i.e., add_count(Therapist, attrition, name = "attr_count)), but this is a good opportunity to using a join function.

```{r}
# Counting the number of patients that stopped comming to each clinician
 attr_rate<-clinician%>%
   count(Therapist, attrition, name = "attr_count") 

# Joining the new attrition count data with the cleaned data
 clean_clinician<-left_join(clean_clinician, attr_rate) 
 
```

**Summary Statistics**

1. Create a separate data frame that summarises the sample's central tendency (i.e., mean and median) and spread (i.e., variance, standard deviation, and mad). 
```{r}
# Notice how a lot of code is repeated. Cases like this is when summarise_if or _at come in handy. See later chapters
summary_stat<-clean_clinician%>%
   select(beck_fun, swb_fun, recovery)%>%
   summarise(beck_fun_m = mean(beck_fun, na.rm = TRUE), # Creating BECK Central Tendency Summaries
             beck_fun_med = median(beck_fun, na.rm = TRUE), 
             beck_fun_sd = sd(beck_fun, na.rm = TRUE), # Creating BECK Spread Summaries
             beck_fun_var = var(beck_fun, na.rm = TRUE), 
             beck_fun_mad = mad(beck_fun, na.rm = TRUE),
             swb_fun_m = mean(swb_fun, na.rm = TRUE), # Creating Central Tendency Summaries
             swb_fun_med = median(swb_fun, na.rm = TRUE), 
             swb_fun_sd = sd(swb_fun, na.rm = TRUE), # Creating Spread Summaries
             swb_fun_var = var(swb_fun, na.rm = TRUE), 
             swb_fun_mad = mad(swb_fun, na.rm = TRUE),
             rec_fun_m = mean(recovery, na.rm = TRUE), # Creating Central Tendency Summaries
             rec_fun_med = median(recovery, na.rm = TRUE), 
             rec_fun_sd = sd(recovery, na.rm = TRUE), # Creating Spread Summaries
             rec_fun_var = var(recovery, na.rm = TRUE), 
             rec_fun_mad = mad(recovery, na.rm = TRUE)
             )


summary_stat
```


2. Create an identical table, except calculate these summary statistics across therapists.
```{r}
grouped_stats<-clean_clinician%>%
  group_by(Therapist)%>%
   select(beck_fun, swb_fun, recovery)%>%
   summarise(beck_fun_m = mean(beck_fun, na.rm = TRUE), # Creating BECK Central Tendency Summaries
             beck_fun_med = median(beck_fun, na.rm = TRUE), 
             beck_fun_sd = sd(beck_fun, na.rm = TRUE), # Creating BECK Spread Summaries
             beck_fun_var = var(beck_fun, na.rm = TRUE), 
             beck_fun_mad = mad(beck_fun, na.rm = TRUE),
             swb_fun_m = mean(swb_fun, na.rm = TRUE), # Creating Central Tendency Summaries
             swb_fun_med = median(swb_fun, na.rm = TRUE), 
             swb_fun_sd = sd(swb_fun, na.rm = TRUE), # Creating Spread Summaries
             swb_fun_var = var(swb_fun, na.rm = TRUE), 
             swb_fun_mad = mad(swb_fun, na.rm = TRUE),
             rec_m = mean(recovery, na.rm = TRUE), # Creating Central Tendency Summaries
             rec_med = median(recovery, na.rm = TRUE), 
             rec_sd = sd(recovery, na.rm = TRUE), # Creating Spread Summaries
             rec_var = var(recovery, na.rm = TRUE), 
             rec_mad = mad(recovery, na.rm = TRUE)
             )


grouped_stats
```
3. Which therapist has the highest attrition rate?

Attrition couldn't be calculated from the cleaned data, because we filtered out those observations!  Since we have the number of people who stopped attending therapy and the number of people who continued to attend therapy, we can recover that information. Again, this could have been more easily solved by using the original clinician data.

This data suggests that Ricardo has the highest attrition rate.
```{r}
 attr_rate<-clean_clinician%>%
              group_by(Therapist)%>%
              summarise(attr_rate = mean(attr_count)/(mean(attr_count)+n()))%>% #Mean can be used because their is no variability within group clinicians.
              arrange(desc(attr_rate))
 attr_rate
 
```



4. Which therapist has the highest recovery rate?

Because recovery rate is stored as a binary variable (i.e., 1 or 0), the proportion of patients recovered is equal to the average of the recovery column. We calculated this above. It suggests that Blaine has the highes recovery rate. 
```{r}
grouped_stats%>%
  select(Therapist, rec_m)%>%
  arrange(desc(rec_m))
```

5. Is there an association between the Recrutiment Medium and recovery rate?

This was an advanced problem. There are several ways to approach this. I illustrate one method below. Using the cleaned data, I calculate the joint, conditional, and marginal probabilities. If the two variables are independent, the conditional probabilities should approximate the marginal probabilities (i.e., the probability of recovery across recruitment channels is equal to the probabilty of recovering overall)
```{r}
# Advanced Question: Calculate joint, conditional, and marginal probabilities 
### Conditonal p should approximate marginal if independent
 clean_clinician%>%
   count(Recruitment_Channel, recovery)%>% # Uses count to count the joint frequency 
   group_by(recovery)%>% 
   mutate(recovery_mf= sum(n))%>% # Calculates the marginal frequencies of recovering and not recovering
   ungroup()%>%
   group_by(Recruitment_Channel)%>%
   mutate(recruitment_mf = sum(n))%>% # Calculates the marginal frequencies of each recruitment channel
   ungroup()%>%
   mutate(p = n/sum(n), # Converting joint frequencies to joint probabilities
          recovery_mp = recovery_mf/sum(n), # Converting marginal frequencies to marginal probabilities
          recruitment_mp = recruitment_mf/sum(n))%>%
  group_by(Recruitment_Channel)%>%
  mutate(recovery_cond_p = n/sum(n))%>% # Calculate probability of recovery conditioned on channel 
  select(Recruitment_Channel, recovery, p, recovery_mp, recovery_cond_p) # Selecting only probabilities
```

**Follow-up Quesitons**

1. What was the difference between scale scores generated by arithmetic operators and functions?
They handle missing data differently. mean() has the na.rm option that allows you to calculate an average using all available data. 


2. Did the information stored within group-level features differ from what was generated using grouped summary statistics?
Nope! They are really generating the same information! The group-level feature extraction just repeats it across observations.


3. When would it be useful to store group-level data as a summary table versus a variable in the original data frame?
It is useful to store group-level data when you want to use it in a model. When you want to communicate data, it is more useful to store it in a separate table. 

*Dplyr Chain with Some Advanced Functions*

```{r}
# Cleaning Data ----------------------------------------------------------------------------------------------------
clean_clinician<-clinician%>%
  filter(attrition!=1)%>% # Removing patients that no longer attend therapy (i.e., attrition == 1)
  select(-Pull_Date, -attrition)%>% # Dropping variables with no variance (Pull_Date and attrition were the same across each obs)
  rename(beck1 = Q1.1, beck2 = Q1.2, beck3 = Q1.3, swb1 = Q2.1, swb2 = Q2.2, swb3 = Q2.3)%>% # renaming to meaningful variables
  filter_at(.vars = vars(beck1, beck2, beck3, swb1, swb2, swb3), .vars_predicate = any_vars(!is.na(.)))%>% # Removing observations if all are missing (retaining if responded on any observations)
  mutate(beck_arith = (beck1+beck2+beck3)/3, swb_arith = (swb1+swb2+swb3)/3)%>% #Using arithmetic methods for means
  rowwise()%>% # Creating scale scores with missing data
  mutate(beck_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE), swb_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE))%>% # Using function for means
  ungroup()%>% # always making sure to ungroup
  mutate(dep_cdf = cume_dist(beck_fun)) # creating a new variable that contains the cdf for becks depression inventory

# Extracting Group-level Features -------------------------------------------------------------------------------------
 clean_clinician<-clean_clinician%>% # Using the cleaned data overwrite clean data.
    group_by(Therapist)%>% # group the data by Therapist
    mutate(dep_cdf_grp = mean(beck_fun, na.rm = TRUE))%>% # calculate the cdf by group
    ungroup()

 # Attrtion rate is no longer stored in this data. Below I show how to use join to add it to the cleaned data
 # A more efficient way would be to create the attr_count variable before I filtered (i.e., add_count(Therapist, attrition, name = "attr_count)), but this is a good opportunity to using a join function

 attr_rate<-clinician%>%
   count(Therapist, attrition, name = "attr_count") # Counting the number of patients that stopped comming to each clinician

 clean_clinician<-left_join(clean_clinician, attr_rate) # Joining the new attrition count data with the cleaned data
 
 # Summary Tables -------------------------------------------------------------------------------------------------
 
 ## summarise_all is useful, and depicted here, but summary() is equally acceptible!
  summary_stat<-clean_clinician%>%
   select(beck_fun, swb_fun, recovery)%>%
   summarise_all(.funs = list(m = ~mean(., na.rm = TRUE),
                              med = ~median(., na.rm = TRUE),
                              sd = ~sd(., na.rm = TRUE),
                              var = ~var(., na.rm = TRUE), 
                              mad = ~mad(., na.rm = TRUE)))
 
 grouped_summary<-clean_clinician%>%
   select(Therapist, beck_fun, swb_fun, recovery)%>%
   group_by(Therapist)%>%
   summarise_all(.funs = list(m = ~mean(., na.rm = TRUE),
                              med = ~median(., na.rm = TRUE),
                              sd = ~sd(., na.rm = TRUE),
                              var = ~var(., na.rm = TRUE), 
                              mad = ~mad(., na.rm = TRUE)))
 
 
  # Printing both summaries
  summary_stat
  grouped_summary
  
 # Attrition couldn't be calculated like recovery rate using the mean function because we filtered out those observations! 
 #Since we have the number of people who stopped attending therapy and the number of people who continued to attend therapy, we can recover that information. 
 # Again, this could have been more easily solved by using the original clinician information. 
 # It had to be calculated separately
  
 attr_rate<-clean_clinician%>%
              group_by(Therapist)%>%
              summarise(attr_rate = mean(attr_count)/(mean(attr_count)+n()))
   
 attr_rate
 
 
 # Based on this information Ricardo has the highest attrition rate
 attr_rate%>% # Using attr_rate data
   select(Therapist, attr_rate)%>% # select the Therapist and attr_rate columns
   arrange(desc(attr_rate)) # sore attr_rate column in descending order
 
 # Blaine has this highest recovery rate
  grouped_summary%>%
    select(Therapist, recovery_m)%>%
    arrange(desc(recovery_m))
 # Relationship between Recruitment Channel and Recovery? -----------------------------------------------  
 clean_clinician%>%
   count(Recruitment_Channel, recovery)%>% # Uses count to count the joint frequency 
   group_by(recovery)%>% 
   mutate(recovery_mf= sum(n))%>% # Calculates the marginal frequencies of recovering and not recovering
   ungroup()%>%
   group_by(Recruitment_Channel)%>%
   mutate(recruitment_mf = sum(n))%>% # Calculates the marginal frequencies of each recruitment channel
   ungroup()%>%
   mutate(p = n/sum(n), # Converting joint frequencies to joint probabilities
          recovery_mp = recovery_mf/sum(n), # Converting marginal frequencies to marginal probabilities
          recruitment_mp = recruitment_mf/sum(n))%>%
  group_by(Recruitment_Channel)%>%
  mutate(recovery_cond_p = n/sum(n))%>% # Calculate probability of recovery conditioned on channel 
  select(Recruitment_Channel, recovery, p, recovery_mp, recovery_cond_p) # Selecting only probabilities
 
```



