[
["index.html", "Psyc 6300: Data Management Chapter 1 Introduction", " Psyc 6300: Data Management James Rigby 2019-08-25 Chapter 1 Introduction "],
["prerequisites.html", "1.1 Prerequisites", " 1.1 Prerequisites This book assumes that you are familiar with the basics of the R language. Thus, we will not discuss basic arithmatic operators, common functions (i.e., mean), or data structures. Please review the material on base R if you are still uncomfortable with the foundations of the language. Datacamp offers a great set of courses (linked here) that will help get you up to speed. If you have yet to do so please install and load tidyverse by running the following code # Install tidyverse install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) "],
["supplemental-resources.html", "1.2 Supplemental Resources", " 1.2 Supplemental Resources This is by no means the only resource to learn data management skills in R. My aim is to provide a somewhat biased overview of how data management should be done in R. I draw heavily on packages from the tidyverse because they result in type consistent output and incorporate piping making them easier to use and interpret when compared to their base R counterparts. Here are additional resources that may provide different perspectives or additional insight into data management in R. Supplemental Resources Dplyr Cheatsheet Dplyr Vignette R For Data Scientists: Chapter 5 DataCamp: Data Manipulation with Dplyr Quick R: Data Management in Base R "],
["acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. "],
["material-overview.html", "Chapter 2 Material Overview", " Chapter 2 Material Overview If you are taking PSYC 6300 with me, this is the lecture plan for the classes covering data management. Day 1: Basic dplyr Part 1: What is dplyr? Part 2: Core dplyr Functions Break Activity 1 Part 3: Bind and Join Functions Break Activity 2 Day 2: Advanced dplyr and tidyr Part 1: Functions for Extracting Observations Part 2: Repeated Operations Break Activity 1 Part 3: spread() and gather() "],
["what-is-dplyr.html", "Chapter 3 What is dplyr?", " Chapter 3 What is dplyr? dplyr is a package that tries to provide a set of functions that utilizes a consistent design, philosophy, grammar, and data structure This consistency increases usability and interpretability of code It is consistently updated and supported by members of the R-Core team and creaters of RStudio It is the most commonly used to manipulate data within the R program "],
["why-is-data-manipulation-important.html", "3.1 Why is Data Manipulation Important?", " 3.1 Why is Data Manipulation Important? 3.1.1 Example 1: Survey Data 3.1.2 What’s Wrong With the Survey Data? Some of the meta-data collected by the survey platform is not meaningful. It is unclear what the data (i.e., Q1.1) is referring to. Items that start with Q1 and Q2 are associated with unique scales that need to be formed into composites. Some observations were created by you during pilot testing and should not be included. "],
["this-isnt-relevant-to-me-my-research-is-experimental.html", "3.2 This Isn’t Relevant to Me - My Research is Experimental!", " 3.2 This Isn’t Relevant to Me - My Research is Experimental! 3.2.1 Example 2: Experimental Data 3.2.2 What’s Wrong With the Experimental Data? Some of your participants figured out the purpose of your expiriment making their responses invalid. Your pre and post scale was miscalibrated and is .3 higher than it should be. Your pre and post measures are stored in seperate data files. Making matters more difficult, you have 17% participant attrition so you can’t just copy and paste data frames together. "],
["take-aways.html", "3.3 Take-Aways", " 3.3 Take-Aways Why Does Data Mangaement Matter? Data is messy, no matter what paradigm you work in. Models have different structuring requirements. Knowing how to use a robust set of tools for data management will save you time. "],
["core-dplyr-functions.html", "Chapter 4 Core dplyr Functions", " Chapter 4 Core dplyr Functions Core dplyr Functions for Data Manipulation filter(): select rows based on some logical condition select(): select columns based on their names rename(): rename columns mutate(): add new variables that are functions of old variables group_by(): perform grouped operations summarise(): create summary statistics for a group arrange(): reorder rows based on some column "],
["form.html", "4.1 dplyr Function Structure", " 4.1 dplyr Function Structure All of the core dplyr functions take the following form: function(data, transformation, …) function: the dplyr function that you want to use data: the data frame or tibble you want to use the function on transformation: the transformation that you want to perform …: other transformations you want to perform "],
["filter.html", "4.2 filter(): Retaining Rows", " 4.2 filter(): Retaining Rows This function allows you to subset the data frame based on a logical test. Simply put, it allows you to choose which rows to keep. 4.2.1 filter() Structure filter(data, logical_test, …) Remember, all dplyr functions take the same general form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case transformation argument uses a logical test to define the observations we would like to keep. Logical tests can explicitly use logical operators (i.e., == or %in%). Functions that return logical values can also be used (i.e., is.na()). Multiple logical tests can be provided as indicated by the ellipse. If tests are separated by a comma or ampersand, both tests must be TRUE for the observation to be retained. If tests are separated by a pipe (i.e., |), either argument can be satisfied for the observation to be retained 4.2.2 Using filter() Remember the survey data? Some observations were created when the survey was being tested. These observations are not informative and should be removed. Luckily, the survey platform records whether a response is from a participant or a tester in the Status column (0 = participant, 8 = tester). Using filter(), we can easily retain the real observations while excluding rows associated with the pilot test. Figure 4.1: Raw Data Example 4.1 Using filter to retain non-pilot observations (Status = 1). filter(survey_data, Status == 0) Figure 4.2: Filtered Data Example 4.2 A less practical example that retains observations that responded to Q1.1 OR Q1.3 with 5 filter(survey_data, Q1.1 == 5 | Q1.3 == 5 ) Figure 4.3: Participants Who Responded 5 to questions Q1.1 OR Q1.3 "],
["select-choosing-columns.html", "4.3 select(): Choosing Columns", " 4.3 select(): Choosing Columns Often when cleaning data, we only want to work with a subset of columns. select() is used to retain or remove specific columns. 4.3.1 select() Structure select(data, cols_to_keep, …) Again, select() takes the general dplyr form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case, the transformation argument specifies a column or columns we would like to keep, separated by commas. If you want to keep a range of columns you can specify the first column and last column of the range with a colon. Sometimes, it is more efficient to drop then select columns. To remove columns, simply include a minus sign in front of the column name. select() can also be used to reorder columns - the columns will be ordered how you type them. 4.3.2 Useful Helper Functions for select() starts_with() used in tandem select() allows you to keep variables that share a stem. ends_with() used in tandem with select() allows you to keep variables that share a suffix. contains() used in tandem with select() allows you to keep variables that share some common string anywhere in their structure. These can be used along with regular expressions to automate large portions of data cleaning. Helper functions can speed up the data cleaning process while keeping your code easy to interpret. 4.3.3 Using select() Again, this function helps us solve two issues in the survey data example. The survey platform created a column of data for the participant’s last name that is completely empty. Furthermore, the Status column is no longer informative because all the values should equal 0. We can remove this column entirely using the select function. All of the following examples complete the same task using different methods although some are more efficient than others! Figure 4.4: Most Recent Data Example 4.3 Using select() by specifying columns to retain. select(survey_data, ResponseId, Q1.1, Q1.2, Q1.3, Q2.1, Q2.2, Q2.3) Figure 4.5: Selected Data Example 4.4 Using select() by specifying columns to omit. select(survey_data, -Status, -last_name) Figure 4.6: Dropped Data Example 4.5 Using select() by specifying range of columns. select(survey_data, ResponseId, Q1.1:Q2.3) Figure 4.7: Range of Variables Selected Example 4.6 Using select() with helper functions. select(survey_data, contains(&quot;id&quot;, ignore.case = TRUE), starts_with(&quot;Q&quot;)) Figure 4.8: Select with Helper Functions "],
["rename-renaming-variables.html", "4.4 rename(): Renaming Variables", " 4.4 rename(): Renaming Variables This function is very self explanatory - it renames columns (variables) 4.4.1 rename() Structure rename(data, old_name = new_name, …) Following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. In this case the transformation arguments take the form of an equation, where the new column name is on the left of the equals sign and the old column name is on the right. Multiple variables can be renamed within one rename call, as indicated by the ellipse. 4.4.2 Using rename() Given that Q1.x and Q2.x are not meaningful stems, we should rename the items so that they are interpretable. It turns out that items that are labeled with the prefix “Q1” measured conscientiousness and items that are measured Q2 measure job performance. Figure 4.9: Most Recent Data Example 4.7 Using rename() to provide substantive column names. rename(survey_data, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Figure 4.10: Renamed Data Example 4.8 select() can be used to rename columns as well! select(survey_data, ResponseId, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Example 4.9 rename() may be one area where dplyr is lacking in efficiency. Here is the base R code to do the same task! colnames(survey_data)&lt;-c(&quot;ResponseId&quot;, paste0(&quot;cons&quot;, 1:3), paste0(&quot;perf&quot;, 1:3)) survey_data Figure 4.11: Renamed Data Using Base R "],
["mutate-creating-new-variables.html", "4.5 mutate(): Creating New Variables", " 4.5 mutate(): Creating New Variables mutate() creates new variables that are defined by some function or operation. 4.5.1 mutate() Structure mutate(data, new_var = function, …) Again, following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. The next argument specifies the transformation, which in mutate() defines a new variable. To do this, you specify a formula that specifies the name of a new variable on the left of the equals sign and a function that creates the new variable on the right. In this notation, function refers to any function or operator that creates a vector of output that is as long as the data frame or has a single value. Multiple new variables can be created within one mutate() call, but should be separated by commas. 4.5.2 Helper Functions rowwise(): Applies functions across columns within rows. ungroup(): Undoes grouping functions such as rowwise() and group_by() (group_by() will be discussed in Section 4.7) 4.5.3 Using mutate() Given that there are two sub scales (i.e., conscientiousness and performance) within our survey data, we can create scale scores for these sets of items. Typically, this is done by averaging the item level data. mutate() provides an easy way to do this! Figure 4.12: Most Recent Data Example 4.10 Using mutate() and arithmetic operators to create scale scores with missing data. mutate(survey_data, cons = (cons1+cons2+cons3)/3, perf = (perf1+perf2+perf3)/3) Figure 4.13: Arithmatic Scale Scores Example 4.11 Using mutate() and rowwise() to create scale scores while handling missing data (use with caution). ungroup(mutate(rowwise(survey_data), cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE))) Figure 4.14: Rowwise Scale Scores Two new functions are used in the code below. percent_rank() calculates the percentage of observations less than an observation. cume_dist() calculates the percentage of obseravtions less than or equal to an observation. Example 4.12 While the above examples illustrate composites, you can also create normalize variables (i.e., percents). mutate(survey_data, perf_p = percent_rank(perf), perf_cdf = cume_dist(perf)) Figure 4.15: Adding Rank Variables This information can be used to provide useful summarise of distributions. I demonstrate how this can be visualized below. p1&lt;-survey_data%&gt;% select(perf, perf_p, perf_cdf)%&gt;% distinct()%&gt;% ggplot(aes(x = perf, y = perf_p))+ geom_density(stat = &quot;identity&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt; x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of percent_ranks() Output&quot;) p2&lt;-survey_data%&gt;% select(perf, perf_p, perf_cdf)%&gt;% distinct()%&gt;% ggplot(aes(x = perf, y = perf_cdf))+ geom_density(stat = &quot;identity&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt;= x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of cume_dist() Output&quot;) ggarrange(p1, p2) Note that, the cume_dist() can be visulized from raw data (without feature engineering) by using the stat_ecdf ecdf stands for empirical cumulative density function survey_data%&gt;% ggplot(aes(x = perf))+ geom_density(stat = &quot;ecdf&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt;= x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of cume_dist() Output&quot;) Example 4.13 You can also create binary indicator variables using conditional logic (i.e. if_else() statements). These indicator variables are sometimes referred to a dummy coded variables or one hot encoding. mutate(survey_data, lt_50p = if_else(perf_p&lt;=.5, 1, 0)) if_else(logical_test, value_if_TRUE, value_if_FALSE) Used to conditionally operate on dataframe Uses two different values or algorithms, depending on a logical test Figure 4.16: Adding Dummy Variables "],
["the-pipe-operator.html", "4.6 The Pipe Operator (%&gt;%)", " 4.6 The Pipe Operator (%&gt;%) Notice that while cleaning the survey data we have been typing the data argument multiple times. Furthermore, using rowwise(), ungroup(), and mutate() all together makes our code difficult to read! Wouldn’t it be nice if there was some shorthand way to link functions together? Lucky for us there is, and it is call the pipe operator. The pipe operator carries forward the output of the previous function and uses it in the function that follows. This allows us to string together multiple functions without retyping the data argument. 4.6.1 Structure of the Pipe Operator function1(data, transformation, …)%&gt;% function2(transformation) The pipe operator carriers forward the output of the previous call and uses it in the subsequent function Thus, following any call with %&gt;% will carry forward the output into the subsequent function The pipe can be used with base R by using a period as a place holder for the data frame. 4.6.2 Using the pipe operator Let’s use the pipe operator to make our code more readable Example 4.14 Using pipe operator (%&gt;%) to redo what we have done thus far. survey_data%&gt;% filter(Status == 0)%&gt;% select(-Status, -last_name)%&gt;% rename(cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3)%&gt;% rowwise()%&gt;% mutate(cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE))%&gt;% ungroup()%&gt;% mutate(perf_p = percent_rank(cons), perf_cdf = cume_dist(perf), lt_50p = if_else(perf_p&lt;=.5, 1, 0))%&gt;% select(matches(&quot;[[:alpha:]]$&quot;)) Figure 4.17: Replicating Data cleaning with Piping "],
["group.html", "4.7 group_by(): Grouping Data Frames", " 4.7 group_by(): Grouping Data Frames Sometimes, when working with data we want to perform some operation within a grouping variable. For example, the participants responding to this survey report to different managers. We may be interested in creating a new column of data that contains the work-groups’ average performance. group_by() can be used in tandem with mutate() to apply a function within columns clustering on groups 4.7.1 group_by Structure group_by(data, grouping_variable, …) group_by() takes the common dplyr structure - define the data and then define the transformation. The transformation in this case simply defines the grouping variable. If multiple grouping variables are provided, the data is grouped by unique combinations of all grouping variables. Note that this function is similar to rowwise() in that no physical change happens to the data - it only affects how later functions act the object. Because of this, group_by() is rarely (dare I say never) used without being accompanied by other functions such as mutate() or summarise() (to be covered in Section 4.8) Also, just like rowwise(), in order return the data set to its ungrouped form it is necessary to call the ungroup() function after finishing grouped manipulations. 4.7.2 Using group_by() The survey data has been joined with information regarding employees managers. We can now calculate each employee’s team’s average performance, conscientiousness, and the number of teammates who responded in the data. While I only illustrate how to use group_by() with the pipe operator, if for some reason you wanted to use a single group_by() call instead of a chain, it can be done. Figure 4.18: Cleaned Data with Manager Info Example 4.15 Using group_by() to create team level variables and n() to create group size variables. survey_data%&gt;% group_by(Manager)%&gt;% mutate(team_cons = mean(cons, na.rm = TRUE), team_size = n())%&gt;% ungroup() Figure 4.19: Group and Size Variables Example 4.16 Using group_by() to create team level cumulative distribution. survey_data%&gt;% group_by(Manager)%&gt;% mutate(team_cdf = cume_dist(perf))%&gt;% ungroup() Figure 4.20: Within-team CDF The CDF can be used to visulize distributional differences across groups as well. Example 4.17 add_count() is a nice alternative, to the group_by()%&gt;%mutate() chain if your goal is to simply add a grouped frequency variables to the data frame. survey_data%&gt;% add_count(Manager) Figure 4.21: Alternative method for creating count variables "],
["sum.html", "4.8 summarise(): Creating Data Summaries", " 4.8 summarise(): Creating Data Summaries While creating grouped variables is sometimes necessary for analyses, often we simply want to describe properties of our data. summarise() is especially useful for this because it applies a function across rows of data to create a single value. If the data is grouped, there is a value returned for each group. 4.8.1 summarise() Structure summarise(data, summary_var = function, …) Following the consistent dplyr structure, summarise() requires that you first specify the data and then a transformation. The transformation in summarise takes a similar form as mutate(). The left hand side of the equation defines the name of a new summary variable and the right hand side defines a function or operation. The function should return a single value (i.e., mean() or sd()). 4.8.2 Using summarise() Let’s create a summary table for the overall sample as well as each team Example 4.18 Using summarise() to create a summary table for the entire survey data frame survey_data%&gt;% summarise(mean_cons = mean(cons, na.rm = TRUE), mean_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Figure 4.22: Summary Statistics Example 4.19 Using group_by() and summarise() to create a summary table for different work groups survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Figure 4.23: Grouped Summary Statistics Example 4.20 count() is a nice alternative to the group_by()%&gt;%summarise() chain if your goal is simply to describe grouped frequencies. survey_data%&gt;% count(Manager) "],
["arrange-ordering-rows.html", "4.9 arrange(): Ordering Rows", " 4.9 arrange(): Ordering Rows arrange() can be used to sort rows in a data frame By default, arrange() orders a data frame from values in a column that go from smallest to largest You can use desc() with arrange to sort from largest to smallest 4.9.1 arrange() Structure arrange(data, sort_var, …) arrange() takes the same structure as all other core dplyr functions. First, specify the data you are manipulating Second, specify the transformation - the column or columns you are sorting by If multiple columns are provided, arrange will sort by the first column and use subsequent columns as tie breakers 4.9.2 Using arrange() Example 4.21 Adding arrange() to our summary table survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(team_cons) Figure 4.24: Sorting the Summary Table Example 4.22 Sorting the summary table in descending order with desc() and arrange() survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(desc(team_cons)) Figure 4.25: Sorting in Descending Order "],
["activity.html", "Chapter 5 Activity", " Chapter 5 Activity Now it’s your turn! You can find a data set titled “clinician.csv” in the supplemental material linked here. A .txt file (clinician_description.txt) is also included describing the data and structure. Your goal is to clean the data and then summarise the data in a meaningful way. Instructions may, at times, be intentionally ambiguous. This is intended to facilitate critical thinking when applying the principles learned. Instructions Data Cleaning Create a dataframe with only the current patients. Remove irrelevant columns. Assign meaningful names to all columns. Remove observatons that have NO data for Beck Depression Inventory and Subjective Well-Being scales. Create scale scores using arithmetic operators. Create scale scores using a method that employs a function to generate the mean. Create a varaible that stores the depression CDF for each participant. Group-Level Feature Extraction Create a variable that stores therapist-level depression scores for each participant Create a variable that stores therapist-level attrition rate. Summary Statistics Create a separate data frame that summarises the sample’s central tendency (i.e., mean, median, and mode) and spread (i.e., variance, standard deviation, and mad). Create an identical table, except calculate these summary statistics across therapists. Which therapist has the highest attrition rate? Which therapist has the highest recovery rate? Is there an association between the Recrutiment Medium and recovery rate? Follow-up Quesitons What was the difference between scale scores generated by arithmetic operators and functions? Did the information stored within group-level features differ than what was generated in the grouped summary statistics? When would it be useful to store group-level data as a summary table versus a variable in the original data frame? "],
["combining-data-sets.html", "Chapter 6 Combining Data Sets", " Chapter 6 Combining Data Sets Take a look the left panel of the tables above which shows the original survey data set after it was cleaned up. Manager information was not originally stored in this data! In order to get manager information into the data frame I did some magic behind the scenes. I merged the survey data set the manager data in the right pane based on the ResponseId variable. Luckily the creators of dplyr wrote a set of functions that make merging multiple data tables easy. In the following sections we are going to learn a variety of different ways to bind and join data frames. "],
["binding-functions.html", "Chapter 7 Binding Functions", " Chapter 7 Binding Functions Binding functions are the most basic method used to combine data sets in the tidyverse, although they are not appropriate for all cases. In the sections that follow we will review what these functions do and highlight cases in which they are and are not appropriate. Binding Functions bind_rows(): Stacks many data frames vertically. bind_cols(): Joins many date frames horizontally. "],
["bind-cols-binding-data-frames-horizontally.html", "7.1 bind_cols(): Binding Data Frames Horizontally", " 7.1 bind_cols(): Binding Data Frames Horizontally bind_cols() is used when you have a set of data frames that Have equal number of rows Are ordered identically, with no missing or new observations If these two requirements are not met, the data will joined combining information about different observations or participants If your data does not meet either of these requirements, but has a participant identifier use a join function discussed below. 7.1.1 bind_cols() Structure bind_cols(…) bind_cols() takes two or more data frames (or a list of data frames) that have an equal number of identically ordered rows. 7.1.2 Using bind_cols() Note that, while for the survey data we want to combine two data frames horizontally, the data frames do not have the same number of rows. Furthermore, based on the responseId variable, we know that participants are not in the same order. Thus we are probably better off using a different function to join these two data sets. In contrast, note that in the two tables below, each data frame has the same number of observations and the ID variables align perfectly. Figure 7.1: Two Measures with Shared Identifier Example 7.1 Binding data frames together horizontally. bind_cols(cons_dat, perf_dat) Figure 7.2: Output of bind_cols Note that, since there is a duplicate column (ResponseId) in the new joined data set, col_bind() automatically added a 1 to the end of the column name. This is intended to prevent mix ups, but can result in duplicate data. While this example was adequate for illustration purposes, in practice, join functions are more flexible and appropriate when data sets have a shared identifier. "],
["bind-rows-binding-data-frames-vertically.html", "7.2 bind_rows(): Binding Data Frames Vertically", " 7.2 bind_rows(): Binding Data Frames Vertically bind_rows() is used when you want to bind data frames vertically. This is sometimes referred to as stacking data frames. Unlike bind_cols(), bind_rows() attempts to match columns based on their names. If a data frame is missing a column, observations will have missing data for that variable. 7.2.1 bind_rows() Structure bind_rows(…) bind_rows() takes two or more data frames (or a list of data frames) 7.2.2 Using bind_rows() Figure 7.3: Two Data Frames with Overlapping and Unique Information Example 7.2 Binding data frames together vertically (AKA, stacking). bind_rows(cons_dat, perf_dat) Figure 7.4: Output of bind_rows() Note that the columns that are named the same are appropriately matched. Furthermore, observations from a data frames that is missing a column are assigned NA for that variable. "],
["mutating-joins.html", "Chapter 8 Mutating Joins", " Chapter 8 Mutating Joins Note that when we were using bind_cols(), corresponding rows in each data frame were assumed to belong to the same observation. This perfect match up rarely occurs unless data frames were programatically spit and manipulated by the user. In psychological research, participant attrition causes some observations to be present in one data set but not another. Furthermore, participants can respond in a different orders across time-points. bind_cols() may also be inadequate when merging data frames associated with different levels in a hierarchy (i.e., team and individual). When merging hierarchical data frames, if one team is associated with many individuals, team information may need to be repeated multiple times. The mutating join functions were developed with these problems in mind. Mutating join functions share a number of common characteristics. They share the same structural form. Data frames are horizontally combined so that the outputted data frame has more columns than either of the independent data frames. Rows are matched based on some common ID variable. If there are multiple rows with the same ID variable, all combinations of rows are returned. Despite their similarities, each mutating join differs in how it handles observations that do not match on an ID variable. In the sections that follow, I will: Define the common join function form. Described how each join function handles observations that do not have a match on the ID variable. Provide examples of uses for each form. List of Mutating Joins left_join(): Joins based on an ID variable. Retains all rows in left data frame and only matching rows in the right. right_join(): Joins based on an ID variable. Retains all rows in the right data frame and only matching rows in the left. inner_join(): Joins based on an ID variable. Retains only matching rows for both data frames. full_join(): Joins based on an ID variable (Considers order). Retains only matching rows for "],
["joinform.html", "8.1 Join Functions: Structural Form", " 8.1 Join Functions: Structural Form function(x, y, by = c(“lh_id” = “rh_id”)) function denotes the type of join you would like to perform (i.e., full_join, left_join). Join functions commonly refer to left-hand and right-hand and data frames. Whether a data frame is a left-hand or right-hand data frame is determined by what order you enter your the two data frames. x defines the left-hand data frame (it is the data frame argument furthest left). y defines the right-hand data frame (it is the data frame argument furthers right). by is an optional argument that defines the ID variables that will be used to match rows lh_id is the ID variable in the left-hand data frame while rh_id is the ID in the right-hand data frame If by is left NULL, join functions will search the for columns that share names and join those. "],
["full-join.html", "8.2 full_join()", " 8.2 full_join() full_join() retains all rows from left- and right-hand data frames. Figure 8.1: Two Data Frames with a Shared Identfier Example 8.1 Joining the survey data with managerial data, retaining all observations from both data frames. How are duplicate matches handled? full_join(cons_dat, manager, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) Figure 8.2: Joining Data Frames using Full_join Example 8.2 Changing which data frame is the left-hand df and which data frame is the right-hand df changes the order of the columns but not which observations are kept. full_join(manager, cons_dat, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) Example 8.3 Since the data frames only share one variable with a commmon name. dplyr does give us a lovely message to let us know what the data frames are being joined by. This message will be suppressed from this point forward. full_join(manager, cons_dat) ## Joining, by = &quot;ResponseId&quot; "],
["left.html", "8.3 left_join()", " 8.3 left_join() left_join() retains all observations in the left-hand data frame but only matching observations from the right-hand data frame. Example 8.4 Retaining all observations from the survey data, but only matching observations from the managerial data. left_join(cons_dat, manager) Example 8.5 For left_join() order does matter. Different observations are retained depending on which data frame is in the left_hand position and which data frame is in the right-hand right hand position. left_join(manager, cons_dat) "],
["right-join.html", "8.4 right_join()", " 8.4 right_join() Unsurprisingly, right_join() is the inverse of left_join(). It simply retains all observations in the right-hand data frame and only matching observations in the left-hand data frame Figure 8.3: Original Data Example 8.6 Note that this example produces output that is identical to Example 8.4. The only difference is the data frame arguments are flipped! right_join(manager, cons_dat) "],
["inner-join.html", "8.5 inner_join()", " 8.5 inner_join() inner_join() only retains observations with matching IDs in both left-hand and right-hand data frames. For our example, there is one respondent that doesn’t have managerial information (ResponseId = R_2Sq8eFhNWEfZOJd). If the purpose of the survey was to provide managers with insight about their team, this person’s responses may not be useful. inner_join() can be used to exclude this person from subsequent reports. Figure 8.4: Original Data Example 8.7 inner_join() will exclude all respondents for whom we do not have managerial information and all employees who did not respond to the survey. inner_join(manager, survey_data) Figure 8.5: Only Retaining Observations that Match "],
["filtering-joins.html", "Chapter 9 Filtering Joins", " Chapter 9 Filtering Joins While mutating joins are useful, sometimes it is necessary to remove observations from a data frame based on information stored elsewhere without adding any information to a focal data frame. Filtering joins do just that. As their title suggests, filtering joins are kind of a hybrid between filter() and the join family of functions. They take the same structural form as mutating joins (discussed in Section 8.1) but remove or retain observations that do not correspond to any observations ID variable in the right-hand data frame. Filtering Joins semi_join(): Retains rows in the left hand data frame that match an ID variable in the right hand data frame. anti_join(): Retains rows in the left hand data frame that do NOT match and ID variable in the right hand data frame. "],
["semi-join.html", "9.1 semi_join()", " 9.1 semi_join() semi_join() retains observations in the left-hand data frame that have corresponding ID variables in the right-hand data frame. No new columns are added to the left-hand data frame. Figure 9.1: Original Data Example 9.1 Using semi_join() to retain observations for which we have managerial data without adding managerial data to the survey data. semi_join(survey_data, manager) "],
["anti-join.html", "9.2 anti_join()", " 9.2 anti_join() anti_join() retains observations in the left-hand data frame that do NOT have corresponding ID variables in the right-hand data frame. No new columns are added to the left-hand data frame. This can be especially useful when trying to identify cases that are not contained in one data frame, but stored in another. This can occur as a result of non-response, participant attrition, or random sampling (as we will see in the next chapter) Figure 9.2: Original Data Example 9.2 Using anti_join() to identify employees in the managerial data frame that have not yet responded to our survey. anti_join(manager, survey_data) Figure 9.3: Using anti_join() to identify employees that have yet to respond to the survey. "],
["activity-1.html", "Chapter 10 Activity", " Chapter 10 Activity Let’s see what you’ve learned! Relational databases are a common way to store data. Information associated with different levels is stored in separate tables. Tables are linked with foreign keys. This organization reduces memory demands, but requires a strong knowledge of joins to extract meaningful information. IMBD stores their data about movies in a similar format. You can find a description of the data files title IMDB.txt in the suppl folder, linked here. Again, instructions may be ambiguous. This is intended to facilitate critical thinking when applying the principles learned. Feel free to use other functions unless explictly told not to. Instructions Data Cleaning Load the following .csv’s into your environment ratings.csv titles.csv principal_actors.csv names.csv Merge the principal_actors and names data frames retaining all observations to create a data frame title cinematic_pros. Create a new object call actors that only contains information about actor-movie pairs where they are categorized as actors. Are there actors that for whom we don’t have their movie history? How many actors are missing movie information? Merge the titles and ratings data sets retaining all observations in title to create a new dataframe called movies. Merge actors and movies using your favorite mutating join. Calculate summary statistics using the actors unique id. Store the below information in an object titled actor_summaries Count the number of movies each actor has been in. Calculate the average ratings for each actor. Create a cumulative distribution variable for the actors’ average ratings. Plot the rating cumulative distribution. Using the actor_summaries and movies, create a data frame that contains the movie information for the actors with ratings in the top 10%. "],
["other-functions-for-extracting-observations.html", "Chapter 11 Other Functions for Extracting Observations", " Chapter 11 Other Functions for Extracting Observations In the previous chapters, we learned several functions that can be used to extract observations from a data frame. filter() uses a logical test to extract observations. In contrast, filtering joins use an ID variable in another data frame to extract observations. In the chapters that follow we will cover functions that allow us to extract observations when we want to Want to take a random subset of observations Want to retain only distinct observations "],
["random-samples-of-observations.html", "11.1 Random Samples of Observations", " 11.1 Random Samples of Observations While very much beyond the scope of this lecture, randomly splitting a data set is a key operation for many statistical procedures. Researchers who want to cross-validated their models subset their data (sometimes \\(n\\) times) to evaluate its performance When a model’s statistical assumptions are violated, a researcher can implement a procedure called bootstrapping that uses resampling methods to estimate a parameters standard error. In short, knowing how to randomly sample a data set opens the door to many other statistical procedures The following sections will define the structural form, and show a few examples, however this section is not ended to go in depth into resampling methods. 11.1.1 Structural Form of sample_ Functions sample_x(data, size, replace, weight, …) sample_x denotes the sampling function you would like to use. data: specifies the data frame you would like to operate on. size: specifies the size of the sample you would like to take either in absolute or relative terms (depending on whether you use sample_n() or sample_frac()). replace: logical value specifying whether a data frame should be sampled with replacement (i.e., bootstrap). weight: a vector of weights equal to the number of observations in the data frame specifying how likely each observation is to be sampled (useful for stratified sampling). 11.1.2 Using sample_n() and sample_frac() sample_frac() and sample_n() only differ in terms of the size argument For sample_frac() you specify a proportion, relative to data argument. For sample_n() you specify an absolute number of rows for the output data. Arguably, sample_frac() is more robust to changes in upstream code. For example, if you catch a mistake in your data cleaning prior to taking a random sample of your data set, sample_frac() will still sample relative to this new data frame. In contrast, sample_n() does not adjust to changes in your code and will still resample based on the n you define. Putting differences aside, lets consider how a researcher could create a training and test data frame to evaluate their model’s performance. When randomly sampling the data frame, always make sure to use set.seed() so that the results are reproducible. I will also include and example that shows you how to work around the potential pitfalls of sample_n() by avoiding hard coding its size argument. Figure 11.1: Original Data Example 11.1 Using sample_n() and anti_join() to create training and test sets, hardcoding size. set.seed(123) training &lt;- sample_n(survey_data, size = 4, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.2: Using sample_n() to randomly sample data Example 11.2 Using sample_frac() and anti_join() to create training and test sets. set.seed(123) training &lt;- sample_frac(survey_data, size = .8, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.3: Using sample_frac to randomly sample data Example 11.3 Using sample_n() and anti_join() to create training and test sets, without hardcoding n. set.seed(123) rel_n&lt;- nrow(survey_data)*.8 training &lt;- sample_n(survey_data, size = rel_n, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.4: Avoiding Hard Coding n "],
["distinct-extracting-unique-observations.html", "11.2 distinct(): extracting unique observations", " 11.2 distinct(): extracting unique observations As we know, data collection methods are not perfect and neither are participants. Sometimes, software accidentally records duplicate observations. Furthermore, participants may take surveys more than once resulting in duplicate information for the same person. Identifying distinct responses is often of critical step in ensuring high fidelity data. The final observation extraction function we will cover provides a means of extracting unique cases. 11.2.1 distinct() Structure distinct(data, distinct_var, …, .keep_all) data: specifies the data frame you would like to operate on. distinct_var: defines a variable for which you would like to identify distinct levels. If multiple variables are provided, distinct identifies unique combinations of these levels. .keep_all: is a logical values that specifies whether or not to keep all other variables in the resulting output. Note that distinct() returns the first observations with a distinct level of distinct_var. This means that if .keep_all = TRUE is only really appropriate when an observation is a true duplicate (all information is redundant). 11.2.2 Using distinct() Figure 11.5: Original Data Let’s take a look at the manager data that we used when combining multiple data frames. Let’s use distinct() create data frames that: Store the names of each manager. Store the IDs associated with each employee. Store ID/manager combinations. Example 11.4 Using distinct() to extract unique levels of manager. manager%&gt;% distinct(Manager) Figure 11.6: Disinct Levels of Manager Example 11.5 Using distinct() to extract unique levels of ResponseId. manager%&gt;% distinct(Employee) Figure 11.7: Unique levels of ResponseId Example 11.6 Using distinct() to extract unique ResponseId-Manager combinations. manager%&gt;% distinct(Employee, Manager) Figure 11.8: Distinct ResponseId-Manager combinations "],
["performing-repeated-operations.html", "Chapter 12 Performing Repeated Operations", " Chapter 12 Performing Repeated Operations By this point, my hope is that you feel comfortable with the core dplyr functions. In the next chapters, we will discuss three variations on each of these functions that allow you to write one line of code to repeat a manipulation many times. While I will not demonstrated every variation of these functions, knowing they exist can help you speed up your data cleaning. What are some instances when you would want to do repeated manipulations? Renaming all all columns so that their names are lower case. Centering a set of independent variables for regression. Calculating summary statistics for a large set of variables. Converting a set of variables that were read as character to numeric. Rounding all numeric variables to the second decimal place. Most of the core functions have variations that variations that facilitate repeated operations in different ways. The names are mostly the same, except a suffix is added to the end to differentiate it from its typical call (i.e., mutate_all()) Each suffix defines the repeated manipulation in a different way. Summary of Suffix Definitions _all: Applies the transformation to all columns. _at: Applies the transformation to a set of columns you define. _if: Applies the transformation to a set of columns that match an argument. "],
["all-suffix.html", "12.1 all() suffix", " 12.1 all() suffix As you might suspect, functions with the _all() suffix apply your specified transformation to all columns in the data frame. This is useful when there is a single transformation that is appropriate for every column. For example, because R is case sensitive, it is much easier to always were with lower case column names. Using rename_all() will help us rename every single column so that it matches this pattern. To use rename_all() we simply define the data frame and then a function that will take a column name and change it in some way. tolower() converts string values to lowercase and is the appropriate function to use here. Figure 12.1: Original Data Example 12.1 Using rename_all() and tolower() to rename all variables so they are lower case. survey_data%&gt;% rename_all(tolower) Figure 12.2: Renamed All variables to lower case "],
["at-suffix.html", "12.2 at() suffix", " 12.2 at() suffix The at suffix applies a given transformation to a set of variables. It relies on the vars() helper function to define these variables. Let’s use the mutate_at() function to center the cons and perf columns In this case, I want to retain my centered and uncentered variables for later use. To do this, I defined a named list that contains the functions I want to apply to the variables I define. The names of the elements in the list will be appended to my original names to create new columns. .s are used as place holders for the vars. Figure 12.3: Original Data Example 12.2 Using mutate_at() to center cons and perf at 0. centered_data&lt;-survey_data%&gt;% mutate_at(vars(cons, perf), list(c = ~ .-mean(., na.rm = TRUE))) centered_data Figure 12.4: Centers Variables Defined in vars() Lets double check our work using summarise_at() Centering a variables changes it’s expected value but not its spread. This means that the new variables should have different means but identical standard deviations compared to the original variables. Example 12.3 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) Figure 12.5: Creates Summary Table for Variables Defined in vars() "],
["if-suffix.html", "12.3 if() suffix", " 12.3 if() suffix The _if suffix applies a transformation to a set of columns that satisfy a logical test. I have been using mutate_if() and round() behind the scenes while writing this book to format most of the tables that you see. Consider Example 12.3 when I don’t use mutate_if(). Example 12.4 The code you saw really generates this ugly beast. centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) Figure 12.6: Unrounded Data The floating point (maximum decimal point) in R goes out a long way making calculations very precise but output unwieldy. By using mutate_if(), we can apply round() repeatedly across the data frame. round() only works with numeric data, though, so we want to avoid applying it character and factor data. Example 12.5 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE)))%&gt;% mutate_if(is.numeric, round, digits = 5) Figure 12.7: Formatting Output using mutate_if "],
["wider-and-longer-data-formats.html", "Chapter 13 Wider and Longer Data Formats", " Chapter 13 Wider and Longer Data Formats You now know how to efficiently add/remove columns, remove rows, and summarise information contained within a data frame. There are a few more tools you need to learn before you can handle most data management tasks. You have seen one form longitudinal data can take in Chapter 1 (separate objects). It can also come in long format and wide format, depicted below. Transitioning back and forth between these formats is an important skill to have, because different analyses require different data formats. For example, most multi-level modeling software take the data in long format. In contrast, many MANCOVA packages require wide format data. Luckily, tidyr, another package written by Hadley Wickham, is specially designed to reshape data. Core tidyr Functions for Reshaping Data gather(): Make a data frame longer spread(): Make a data frame wider Figure 13.1: Wide Format Figure 13.2: Long Format "],
["gather-wider-to-longer.html", "13.1 gather(): Wider to Longer", " 13.1 gather(): Wider to Longer gather() is a function intended to take wide format data and make it longer. It does this by gathering a set of columns in the wide data into one column. The user defines the columns columns that should be collapsed, the name of the column that will store the original columns’ names, and the name of the column that will store the original columns’ values. 13.1.1 gather() Structure gather(data, key, value, …) key: Name of column to store wide format column names. value: Name of column to store the selected columns values. …: A selection of columns to gather into long format. This operates similar to select() and can accommodate special operators like :, -, starts_with, etc. 13.1.2 Using gather() gather() is useful for converting a data frame into a longer format. We will illustrate this with the wide data set shown previously This data frame has to columns for a test core taken at two different time points These are labeled “pre” and “post” gather() can be used to stack the pre and post columns into a single test column which we will define with the value The temporal information can be stored in a separate column which we define with the key argument Figure 13.3: Wide Format Data Example 13.1 We can specify the columns we want to gather using the elipse argument gather(wide, key = &quot;time&quot;, value = &quot;test&quot;, pre, post) Figure 13.4: Using gather() to Go From Wide to Long Example 13.2 Equivalent sytax would be to use - to deselect columns to be gathered. In some use cases, this is quicker. gather(wide, key = &quot;time&quot;, value = &quot;test&quot;, -c(id, gender, condition, non_naieve)) "],
["spread-longer-to-wider.html", "13.2 spread(): Longer to Wider", " 13.2 spread(): Longer to Wider spread() is gather()’s conjugate. It converts long data frames into wider data frames. To do this, the user specifies a key column, that stores variable names, and a value column that stores the information associated with those variables. The key column is spread so that each unique value stored within it becomes a new column storing the associated values. 13.2.1 spread() Structure spread(data, key, value) key: Name of column to store wide format column names. value: Name of column to store the selected columns values. 13.2.2 Using spread() Figure 13.5: Long Data Example 13.3 spread(wide, key = &quot;time&quot;, value = &quot;test&quot;) Figure 13.6: Using Spread to Go From Long to Wide "],
["recent-developments.html", "13.3 Recent Developments", " 13.3 Recent Developments The tidyverse is under constant development by a team of very smart people. Recently, Hadley Wickham, one of the key contributes to the tidyverse announced new functions that will come to replace gather() and spread. To prepare you for this eventuality, I am going to introduce pivot_wider() and pivot_longer(). Given that they are still in development, I will only cover the very basics of these functions. They are only available in the tidyr development version and are not yet available in the tidyverse package To download the development version restart your R session (ctr+shift+f10) and run the following code install.packages(&quot;devtools&quot;) devtools::install_github(&quot;tidyverse/tidyr&quot;) 13.3.1 The Problems with gather() and spread() Gather and spread will throw errors when working with some data types. Some find the functions non-intuitive. The latest iteration of these functions make them more robust and also more intuitive. Cannot extract meta-data from column names. "],
["pivot-longer-gathers-predecessor.html", "13.4 pivot_longer(): gather’s() predecessor", " 13.4 pivot_longer(): gather’s() predecessor pivot_longer() is the most recent iteration of gather() It takes a data set and transforms it so it is longer, just like gather(). 13.4.1 pivot_longer() structure pivot_longer(data, names_to, values_to, cols, …) cols: The columns to gather, defined similar to select() and gather() names_to: Column name to store the former column names. values_to: Column name to store the former value names. …: to see additional arguments run ?pivot_longer. 13.4.2 Using pivot_longer() Figure 13.7: Wide Data Example 13.4 We can specify the columns we want to gather using the elipse argument pivot_longer(wide, cols = c(pre, post), key = &quot;time&quot;, value = &quot;test&quot;) Figure 13.8: Using pivot_longer() to go from Wide to Long "],
["pivot-wider-spreads-predecessor.html", "13.5 pivot_wider(): spread()’s predecessor", " 13.5 pivot_wider(): spread()’s predecessor pivot_wider() is the most recent iteration of spread(). pivot_wider() takes a data frame and makes it wider. 13.5.1 pivot_wider() Structure pivot_wider(data, names_from, values_from, …) names_from: Column name to store the former column names. values_from: Column name to store the former value names. …: to see additional arguments run ?pivot_longer. Figure 13.9: Long Data Example 13.3 pivot_wider(long, names_from = &quot;time&quot;, values_from = &quot;test&quot;) Figure 13.10: Using pivot_wider() to Go From Long to Wide "]
]
