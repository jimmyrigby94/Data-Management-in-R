[
["index.html", "Psyc 6300: Data Management Chapter 1 Introduction", " Psyc 6300: Data Management James Rigby 2019-10-12 Chapter 1 Introduction "],
["prerequisites.html", "1.1 Prerequisites", " 1.1 Prerequisites This book assumes that you are familiar with the basics of the R language. Thus, we will not discuss basic arithmatic operators, common functions (i.e., mean), or data structures. Please review the material on base R if you are still uncomfortable with the foundations of the language. Datacamp offers a great set of courses (linked here) that will help get you up to speed. If you have yet to do so please install and load tidyverse by running the following code # Install tidyverse install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) "],
["supplemental-resources.html", "1.2 Supplemental Resources", " 1.2 Supplemental Resources This is by no means the only resource to learn data management skills in R. My aim is to provide a somewhat biased overview of how data management should be done in R. I draw heavily on packages from the tidyverse because they result in type consistent output and incorporate piping making them easier to use and interpret when compared to their base R counterparts. Here are additional resources that may provide different perspectives or additional insight into data management in R. Supplemental Resources Dplyr Cheatsheet Dplyr Vignette R For Data Scientists: Chapter 5 DataCamp: Data Manipulation with Dplyr Quick R: Data Management in Base R "],
["acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. "],
["material-overview.html", "Chapter 2 Material Overview", " Chapter 2 Material Overview If you are taking PSYC 6300 with me, this is the lecture plan for the classes covering data management. Day 1: Basic dplyr Part 1: What is dplyr? Part 2: Core dplyr Functions Break Activity 1 Part 3: Bind and Join Functions Break Activity 2 Day 2: Advanced dplyr and tidyr Part 1: Functions for Extracting Observations Part 2: Repeated Operations Break Activity 1 Part 3: spread() and gather() "],
["what-is-dplyr.html", "Chapter 3 What is dplyr?", " Chapter 3 What is dplyr? dplyr is a package that tries to provide a set of functions that utilizes a consistent design, philosophy, grammar, and data structure This consistency increases usability and interpretability of code It is consistently updated and supported by members of the R-Core team and creaters of RStudio It is the most commonly used to manipulate data within the R program "],
["why-is-data-manipulation-important.html", "3.1 Why is Data Manipulation Important?", " 3.1 Why is Data Manipulation Important? 3.1.1 Example 1: Survey Data 3.1.2 What’s Wrong With the Survey Data? Some of the meta-data collected by the survey platform is not meaningful. It is unclear what the data (i.e., Q1.1) is referring to. Items that start with Q1 and Q2 are associated with unique scales that need to be formed into composites. Some observations were created by you during pilot testing and should not be included. "],
["this-isnt-relevant-to-me-my-research-is-experimental.html", "3.2 This Isn’t Relevant to Me - My Research is Experimental!", " 3.2 This Isn’t Relevant to Me - My Research is Experimental! 3.2.1 Example 2: Experimental Data 3.2.2 What’s Wrong With the Experimental Data? Some of your participants figured out the purpose of your expiriment making their responses invalid. Your pre and post scale was miscalibrated and is .3 higher than it should be. Your pre and post measures are stored in seperate data files. Making matters more difficult, you have 17% participant attrition so you can’t just copy and paste data frames together. "],
["take-aways.html", "3.3 Take-Aways", " 3.3 Take-Aways Why Does Data Mangaement Matter? Data is messy, no matter what paradigm you work in. Models have different structuring requirements. Knowing how to use a robust set of tools for data management will save you time. "],
["core-dplyr-functions.html", "Chapter 4 Core dplyr Functions", " Chapter 4 Core dplyr Functions Core dplyr Functions for Data Manipulation filter(): select rows based on some logical condition select(): select columns based on their names rename(): rename columns mutate(): add new variables that are functions of old variables group_by(): perform grouped operations summarise(): create summary statistics for a group arrange(): reorder rows based on some column "],
["form.html", "4.1 dplyr Function Structure", " 4.1 dplyr Function Structure All of the core dplyr functions take the following form: function(data, transformation, …) function: the dplyr function that you want to use data: the data frame or tibble you want to use the function on transformation: the transformation that you want to perform …: other transformations you want to perform "],
["filter.html", "4.2 filter(): Retaining Rows", " 4.2 filter(): Retaining Rows This function allows you to subset the data frame based on a logical test. Simply put, it allows you to choose which rows to keep. 4.2.1 filter() Structure filter(data, logical_test, …) Remember, all dplyr functions take the same general form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case transformation argument uses a logical test to define the observations we would like to keep. Logical tests can explicitly use logical operators (i.e., == or %in%). Functions that return logical values can also be used (i.e., is.na()). Multiple logical tests can be provided as indicated by the ellipse. If tests are separated by a comma or ampersand, both tests must be TRUE for the observation to be retained. If tests are separated by a pipe (i.e., |), either argument can be satisfied for the observation to be retained 4.2.2 Using filter() Remember the survey data? Some observations were created when the survey was being tested. These observations are not informative and should be removed. Luckily, the survey platform records whether a response is from a participant or a tester in the Status column (0 = participant, 8 = tester). Using filter(), we can easily retain the real observations while excluding rows associated with the pilot test. Figure 4.1: Raw Data Example 4.1 Using filter to retain non-pilot observations (Status = 0). filter(survey_data, Status == 0) Figure 4.2: Filtered Data Example 4.2 A less practical example that retains observations that responded to Q1.1 OR Q1.3 with 5 filter(survey_data, Q1.1 == 5 | Q1.3 == 5 ) Figure 4.3: Participants Who Responded 5 to questions Q1.1 OR Q1.3 "],
["select-choosing-columns.html", "4.3 select(): Choosing Columns", " 4.3 select(): Choosing Columns Often when cleaning data, we only want to work with a subset of columns. select() is used to retain or remove specific columns. 4.3.1 select() Structure select(data, cols_to_keep, …) Again, select() takes the general dplyr form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case, the transformation argument specifies a column or columns we would like to keep, separated by commas. If you want to keep a range of columns you can specify the first column and last column of the range with a colon. Sometimes, it is more efficient to drop then select columns. To remove columns, simply include a minus sign in front of the column name. select() can also be used to reorder columns - the columns will be ordered how you type them. 4.3.2 Useful Helper Functions for select() starts_with() used in tandem select() allows you to keep variables that share a stem. ends_with() used in tandem with select() allows you to keep variables that share a suffix. contains() used in tandem with select() allows you to keep variables that share some common string anywhere in their structure. These can be used along with regular expressions to automate large portions of data cleaning. Helper functions can speed up the data cleaning process while keeping your code easy to interpret. 4.3.3 Using select() Again, this function helps us solve two issues in the survey data example. The survey platform created a column of data for the participant’s last name that is completely empty. Furthermore, the Status column is no longer informative because all the values should equal 0. We can remove this column entirely using the select function. All of the following examples complete the same task using different methods although some are more efficient than others! Figure 4.4: Most Recent Data Example 4.3 Using select() by specifying columns to retain. select(survey_data, ResponseId, Q1.1, Q1.2, Q1.3, Q2.1, Q2.2, Q2.3) Figure 4.5: Selected Data Example 4.4 Using select() by specifying columns to omit. select(survey_data, -Status, -last_name) Figure 4.6: Dropped Data Example 4.5 Using select() by specifying range of columns. select(survey_data, ResponseId, Q1.1:Q2.3) Figure 4.7: Range of Variables Selected Example 4.6 Using select() with helper functions. select(survey_data, contains(&quot;id&quot;, ignore.case = TRUE), starts_with(&quot;Q&quot;)) Figure 4.8: Select with Helper Functions "],
["rename-renaming-variables.html", "4.4 rename(): Renaming Variables", " 4.4 rename(): Renaming Variables This function is very self explanatory - it renames columns (variables) 4.4.1 rename() Structure rename(data, new_name = old_name, …) Following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. In this case the transformation arguments take the form of an equation, where the new column name is on the left of the equals sign and the old column name is on the right. Multiple variables can be renamed within one rename call, as indicated by the ellipse. 4.4.2 Using rename() Given that Q1.x and Q2.x are not meaningful stems, we should rename the items so that they are interpretable. It turns out that items that are labeled with the prefix “Q1” measured conscientiousness and items that are measured Q2 measure job performance. Figure 4.9: Most Recent Data Example 4.7 Using rename() to provide substantive column names. rename(survey_data, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Figure 4.10: Renamed Data Example 4.8 select() can be used to rename columns as well! select(survey_data, ResponseId, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Example 4.9 rename() may be one area where dplyr is lacking in efficiency. Here is the base R code to do the same task! colnames(survey_data)&lt;-c(&quot;ResponseId&quot;, paste0(&quot;cons&quot;, 1:3), paste0(&quot;perf&quot;, 1:3)) survey_data Figure 4.11: Renamed Data Using Base R "],
["mutate-creating-new-variables.html", "4.5 mutate(): Creating New Variables", " 4.5 mutate(): Creating New Variables mutate() creates new variables that are defined by some function or operation. 4.5.1 mutate() Structure mutate(data, new_var = function, …) Again, following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. The next argument specifies the transformation, which in mutate() defines a new variable. To do this, you specify a formula that specifies the name of a new variable on the left of the equals sign and a function that creates the new variable on the right. In this notation, function refers to any function or operator that creates a vector of output that is as long as the data frame or has a single value. Multiple new variables can be created within one mutate() call, but should be separated by commas. 4.5.2 Helper Functions rowwise(): Applies functions across columns within rows. ungroup(): Undoes grouping functions such as rowwise() and group_by() (group_by() will be discussed in Section 4.7) 4.5.3 Using mutate() Given that there are two sub scales (i.e., conscientiousness and performance) within our survey data, we can create scale scores for these sets of items. Typically, this is done by averaging the item level data. mutate() provides an easy way to do this! Figure 4.12: Most Recent Data Example 4.10 Using mutate() and arithmetic operators to create scale scores with missing data. mutate(survey_data, cons = (cons1+cons2+cons3)/3, perf = (perf1+perf2+perf3)/3) Figure 4.13: Arithmatic Scale Scores Example 4.11 Using mutate() and rowwise() to create scale scores while handling missing data (use with caution). ungroup( mutate(rowwise(survey_data), cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE) ) ) Figure 4.14: Rowwise Scale Scores Two new functions are used in the code below. percent_rank() calculates the percentage of observations less than an observation. cume_dist() calculates the percentage of obseravtions less than or equal to an observation. Example 4.12 While the above examples illustrate composites, you can also create normalize variables (i.e., percents). mutate(survey_data, perf_p = percent_rank(perf), perf_cdf = cume_dist(perf)) Figure 4.15: Adding Rank Variables This information can be used to provide useful summarise of distributions. I demonstrate how this can be visualized below. p1&lt;-survey_data%&gt;% select(perf, perf_p, perf_cdf)%&gt;% distinct()%&gt;% ggplot(aes(x = perf, y = perf_p))+ geom_density(stat = &quot;identity&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt; x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of percent_ranks() Output&quot;) p2&lt;-survey_data%&gt;% select(perf, perf_p, perf_cdf)%&gt;% distinct()%&gt;% ggplot(aes(x = perf, y = perf_cdf))+ geom_density(stat = &quot;identity&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt;= x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of cume_dist() Output&quot;) ggarrange(p1, p2) Note that, the cume_dist() can be visulized from raw data (without feature engineering) by using the stat_ecdf ecdf stands for empirical cumulative density function survey_data%&gt;% ggplot(aes(x = perf))+ geom_density(stat = &quot;ecdf&quot;, fill = &quot;blue&quot;, color = &quot;white&quot;)+ scale_x_continuous(name = &quot;Performance&quot;, breaks = 1:5, labels = 1:5, limits = c(1, 5))+ scale_y_continuous(name = &quot;p(Performance&lt;= x)&quot;, breaks = c(0, .25, .5, .75, 1), labels = paste0(c(0, .25, .5, .75, 1)*100, &quot;%&quot;))+ labs(title = &quot;Plot of cume_dist() Output&quot;) Example 4.13 You can also create binary indicator variables using conditional logic (i.e. if_else() statements). These indicator variables are sometimes referred to a dummy coded variables or one hot encoding. mutate(survey_data, lte_50p = if_else(perf_p&lt;=.5, 1, 0)) if_else(logical_test, value_if_TRUE, value_if_FALSE) Used to conditionally operate on dataframe Uses two different values or algorithms, depending on a logical test Figure 4.16: Adding Dummy Variables "],
["the-pipe-operator.html", "4.6 The Pipe Operator (%&gt;%)", " 4.6 The Pipe Operator (%&gt;%) Notice that while cleaning the survey data we have been typing the data argument multiple times. Furthermore, using rowwise(), ungroup(), and mutate() all together makes our code difficult to read! Wouldn’t it be nice if there was some shorthand way to link functions together? Lucky for us there is, and it is call the pipe operator. The pipe operator carries forward the output of the previous function and uses it in the function that follows. This allows us to string together multiple functions without retyping the data argument. 4.6.1 Structure of the Pipe Operator function1(data, transformation, …)%&gt;% function2(transformation) The pipe operator carriers forward the output of the previous call and uses it in the subsequent function Thus, following any call with %&gt;% will carry forward the output into the subsequent function The pipe can be used with base R by using a period as a place holder for the data frame. 4.6.2 Using the pipe operator Let’s use the pipe operator to make our code more readable Example 4.14 Using pipe operator (%&gt;%) to redo what we have done thus far. survey_data%&gt;% filter(Status == 0)%&gt;% select(-Status, -last_name)%&gt;% rename(cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3)%&gt;% rowwise()%&gt;% mutate(cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE))%&gt;% ungroup()%&gt;% mutate(perf_p = percent_rank(perf), perf_cdf = cume_dist(perf), lte_50p = if_else(perf_p&lt;=.5, 1, 0))%&gt;% select(matches(&quot;[[:alpha:]]$&quot;)) Figure 4.17: Replicating Data cleaning with Piping "],
["group.html", "4.7 group_by(): Grouping Data Frames", " 4.7 group_by(): Grouping Data Frames Sometimes, when working with data we want to perform some operation within a grouping variable. For example, the participants responding to this survey report to different managers. We may be interested in creating a new column of data that contains the work-groups’ average performance. group_by() can be used in tandem with mutate() to apply a function within columns clustering on groups 4.7.1 group_by Structure group_by(data, grouping_variable, …) group_by() takes the common dplyr structure - define the data and then define the transformation. The transformation in this case simply defines the grouping variable. If multiple grouping variables are provided, the data is grouped by unique combinations of all grouping variables. Note that this function is similar to rowwise() in that no physical change happens to the data - it only affects how later functions act the object. Because of this, group_by() is rarely (dare I say never) used without being accompanied by other functions such as mutate() or summarise() (to be covered in Section 4.8) Also, just like rowwise(), in order return the data set to its ungrouped form it is necessary to call the ungroup() function after finishing grouped manipulations. 4.7.2 Using group_by() The survey data has been joined with information regarding employees managers. We can now calculate each employee’s team’s average performance, conscientiousness, and the number of teammates who responded in the data. While I only illustrate how to use group_by() with the pipe operator, if for some reason you wanted to use a single group_by() call instead of a chain, it can be done. Figure 4.18: Cleaned Data with Manager Info Example 4.15 Using group_by() to create team level variables and n() to create group size variables. survey_data%&gt;% group_by(Manager)%&gt;% mutate(team_cons = mean(cons, na.rm = TRUE), team_size = n())%&gt;% ungroup() Figure 4.19: Group and Size Variables Example 4.16 Using group_by() to create team level cumulative distribution. survey_data%&gt;% group_by(Manager)%&gt;% mutate(team_cdf = cume_dist(perf))%&gt;% ungroup() Figure 4.20: Within-team CDF The CDF can be used to visulize distributional differences across groups as well. Example 4.17 add_count() is a nice alternative, to the group_by()%&gt;%mutate() chain if your goal is to simply add a grouped frequency variables to the data frame. survey_data%&gt;% add_count(Manager) Figure 4.21: Alternative method for creating count variables "],
["sum.html", "4.8 summarise(): Creating Data Summaries", " 4.8 summarise(): Creating Data Summaries While creating grouped variables is sometimes necessary for analyses, often we simply want to describe properties of our data. summarise() is especially useful for this because it applies a function across rows of data to create a single value. If the data is grouped, there is a value returned for each group. 4.8.1 summarise() Structure summarise(data, summary_var = function, …) Following the consistent dplyr structure, summarise() requires that you first specify the data and then a transformation. The transformation in summarise takes a similar form as mutate(). The left hand side of the equation defines the name of a new summary variable and the right hand side defines a function or operation. The function should return a single value (i.e., mean() or sd()). 4.8.2 Using summarise() Let’s create a summary table for the overall sample as well as each team Example 4.18 Using summarise() to create a summary table for the entire survey data frame survey_data%&gt;% summarise(mean_cons = mean(cons, na.rm = TRUE), mean_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Figure 4.22: Summary Statistics Example 4.19 Using group_by() and summarise() to create a summary table for different work groups survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Figure 4.23: Grouped Summary Statistics Example 4.20 count() is a nice alternative to the group_by()%&gt;%summarise() chain if your goal is simply to describe grouped frequencies. survey_data%&gt;% count(Manager) "],
["arrange-ordering-rows.html", "4.9 arrange(): Ordering Rows", " 4.9 arrange(): Ordering Rows arrange() can be used to sort rows in a data frame By default, arrange() orders a data frame from values in a column that go from smallest to largest You can use desc() with arrange to sort from largest to smallest 4.9.1 arrange() Structure arrange(data, sort_var, …) arrange() takes the same structure as all other core dplyr functions. First, specify the data you are manipulating Second, specify the transformation - the column or columns you are sorting by If multiple columns are provided, arrange will sort by the first column and use subsequent columns as tie breakers 4.9.2 Using arrange() Example 4.21 Adding arrange() to our summary table survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(team_cons) Figure 4.24: Sorting the Summary Table Example 4.22 Sorting the summary table in descending order with desc() and arrange() survey_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(desc(team_cons)) Figure 4.25: Sorting in Descending Order "],
["activity.html", "Chapter 5 Activity", " Chapter 5 Activity Now it’s your turn! You can find a data set titled “clinician.csv” in the supplemental material linked here. A .txt file (clinician_description.txt) is also included describing the data and structure. Your goal is to clean the data and then summarise the data in a meaningful way. Instructions may, at times, be intentionally ambiguous. This is intended to facilitate critical thinking when applying the principles learned. Instructions Data Cleaning Create a dataframe with only the current patients. Remove irrelevant columns. Assign meaningful names to all columns. Remove observatons that have NO data for Beck Depression Inventory and Subjective Well-Being scales. Create scale scores using arithmetic operators. Create scale scores using a method that employs a function to generate the mean. Create a varaible that stores the depression CDF for each participant. Group-Level Feature Extraction Create a variable that stores therapist-level depression scores for each participant Create a variable that stores therapist-level attrition rate. Summary Statistics Create a separate data frame that summarises the sample’s central tendency (i.e., mean and median) and spread (i.e., variance, standard deviation, and mad). Create an identical table, except calculate these summary statistics across therapists. Which therapist has the highest attrition rate? Which therapist has the highest recovery rate? Is there an association between the Recrutiment Medium and recovery rate? Follow-up Quesitons What was the difference between scale scores generated by arithmetic operators and functions? Did the information stored within group-level features differ from what was generated using grouped summary statistics? When would it be useful to store group-level data as a summary table versus a variable in the original data frame? "],
["solutions.html", "5.1 Solutions", " 5.1 Solutions Below are the solutions for the activity. First I answer each question individually. After that, I use a single dplyr chain to complete the Data Cleaning and Group-level Feature Extraction exercises. For each function call in the chain, I explain what the functions are doing. Note that functions ending in _at(), _all(), or _if are advanced function that save time. The associated base calls are just as appropriate. Data Cleaning # Preporation work ---------------------- library(tidyverse) # Read the data clinician&lt;-read_csv(&quot;suppl/clinician.csv&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] Create a dataframe with only the current patients. clean_clinician&lt;-clinician%&gt;% filter(attrition!=1) # Removing patients that no longer attend therapy (i.e., attrition == 1) Remove irrelevant columns. # Dropping variables with no variance (Pull_Date and # attrition were the same across each obs) clean_clinician&lt;-clean_clinician%&gt;% select(-Pull_Date, -attrition) Assign meaningful names to all columns. # renaming to meaningful variables clean_clinician&lt;-clean_clinician%&gt;% rename(beck1 = Q1.1, beck2 = Q1.2, beck3 = Q1.3, swb1 = Q2.1, swb2 = Q2.2, swb3 = Q2.3) Remove observatons that have NO data for Beck Depression Inventory and Subjective Well-Being scales. # Removing observations if all are missing (retaining if responded on any observations) clean_clinician&lt;-clean_clinician%&gt;% filter(!is.na(beck1)|!is.na(beck2)|!is.na(beck3)| !is.na(swb1)|!is.na(swb2)|!is.na(swb3)) Create scale scores using arithmetic operators. #Creating a new column called beck_arith using arithmetic methods for means clean_clinician&lt;-clean_clinician%&gt;% mutate(beck_arith = (beck1+beck2+beck3)/3, swb_arith = (swb1+swb2+swb3)/3) Create scale scores using a method that employs a function to generate the mean. clean_clinician&lt;-clean_clinician%&gt;% rowwise()%&gt;% # Creating scale scores with missing data mutate(beck_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE), swb_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE))%&gt;% # Using function for means ungroup()# always making sure to ungroup Create a varaible that stores the depression CDF for each participant. # creating a new variable that contains the cdf for becks depression inventory clean_clinician&lt;-clean_clinician%&gt;% mutate(dep_cdf = cume_dist(beck_fun)) Group-Level Feature Extraction Create a variable that stores therapist-level depression scores for each participant clean_clinician&lt;-clean_clinician%&gt;% # Using the cleaned data overwrite clean data. group_by(Therapist)%&gt;% # group the data by Therapist mutate(dep_cdf_grp = mean(beck_fun, na.rm = TRUE))%&gt;% # calculate the cdf by group ungroup() Create a variable that stores therapist-level attrition rate. Attrtion rate is no longer stored in this data. We deselected it above. Below, I show how to use join to add it to the cleaned data. A more efficient way would be to create the attr_count variable before I filtered out former patients. (i.e., add_count(Therapist, attrition, name = \"attr_count)), but this is a good opportunity to using a join function. # Counting the number of patients that stopped comming to each clinician attr_rate&lt;-clinician%&gt;% count(Therapist, attrition, name = &quot;attr_count&quot;) # Joining the new attrition count data with the cleaned data clean_clinician&lt;-left_join(clean_clinician, attr_rate) Summary Statistics Create a separate data frame that summarises the sample’s central tendency (i.e., mean and median) and spread (i.e., variance, standard deviation, and mad). # Notice how a lot of code is repeated. Cases like this is when summarise_if or _at come in handy. See later chapters summary_stat&lt;-clean_clinician%&gt;% select(beck_fun, swb_fun, recovery)%&gt;% summarise(beck_fun_m = mean(beck_fun, na.rm = TRUE), # Creating BECK Central Tendency Summaries beck_fun_med = median(beck_fun, na.rm = TRUE), beck_fun_sd = sd(beck_fun, na.rm = TRUE), # Creating BECK Spread Summaries beck_fun_var = var(beck_fun, na.rm = TRUE), beck_fun_mad = mad(beck_fun, na.rm = TRUE), swb_fun_m = mean(swb_fun, na.rm = TRUE), # Creating Central Tendency Summaries swb_fun_med = median(swb_fun, na.rm = TRUE), swb_fun_sd = sd(swb_fun, na.rm = TRUE), # Creating Spread Summaries swb_fun_var = var(swb_fun, na.rm = TRUE), swb_fun_mad = mad(swb_fun, na.rm = TRUE), rec_fun_m = mean(recovery, na.rm = TRUE), # Creating Central Tendency Summaries rec_fun_med = median(recovery, na.rm = TRUE), rec_fun_sd = sd(recovery, na.rm = TRUE), # Creating Spread Summaries rec_fun_var = var(recovery, na.rm = TRUE), rec_fun_mad = mad(recovery, na.rm = TRUE) ) summary_stat ## # A tibble: 1 x 15 ## beck_fun_m beck_fun_med beck_fun_sd beck_fun_var beck_fun_mad swb_fun_m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.19 4.33 1.40 1.95 1.48 4.19 ## # ... with 9 more variables: swb_fun_med &lt;dbl&gt;, swb_fun_sd &lt;dbl&gt;, ## # swb_fun_var &lt;dbl&gt;, swb_fun_mad &lt;dbl&gt;, rec_fun_m &lt;dbl&gt;, ## # rec_fun_med &lt;dbl&gt;, rec_fun_sd &lt;dbl&gt;, rec_fun_var &lt;dbl&gt;, ## # rec_fun_mad &lt;dbl&gt; Create an identical table, except calculate these summary statistics across therapists. grouped_stats&lt;-clean_clinician%&gt;% group_by(Therapist)%&gt;% select(beck_fun, swb_fun, recovery)%&gt;% summarise(beck_fun_m = mean(beck_fun, na.rm = TRUE), # Creating BECK Central Tendency Summaries beck_fun_med = median(beck_fun, na.rm = TRUE), beck_fun_sd = sd(beck_fun, na.rm = TRUE), # Creating BECK Spread Summaries beck_fun_var = var(beck_fun, na.rm = TRUE), beck_fun_mad = mad(beck_fun, na.rm = TRUE), swb_fun_m = mean(swb_fun, na.rm = TRUE), # Creating Central Tendency Summaries swb_fun_med = median(swb_fun, na.rm = TRUE), swb_fun_sd = sd(swb_fun, na.rm = TRUE), # Creating Spread Summaries swb_fun_var = var(swb_fun, na.rm = TRUE), swb_fun_mad = mad(swb_fun, na.rm = TRUE), rec_m = mean(recovery, na.rm = TRUE), # Creating Central Tendency Summaries rec_med = median(recovery, na.rm = TRUE), rec_sd = sd(recovery, na.rm = TRUE), # Creating Spread Summaries rec_var = var(recovery, na.rm = TRUE), rec_mad = mad(recovery, na.rm = TRUE) ) grouped_stats ## # A tibble: 5 x 16 ## Therapist beck_fun_m beck_fun_med beck_fun_sd beck_fun_var beck_fun_mad ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blaine 3.97 4 1.43 2.04 1.48 ## 2 Dustin 4.21 4.33 1.41 2.00 1.48 ## 3 Nikola 4.29 4.33 1.34 1.80 1.48 ## 4 Ricardo 4.18 4.33 1.32 1.75 1.48 ## 5 Samantha 4.28 4.33 1.46 2.12 1.48 ## # ... with 10 more variables: swb_fun_m &lt;dbl&gt;, swb_fun_med &lt;dbl&gt;, ## # swb_fun_sd &lt;dbl&gt;, swb_fun_var &lt;dbl&gt;, swb_fun_mad &lt;dbl&gt;, rec_m &lt;dbl&gt;, ## # rec_med &lt;dbl&gt;, rec_sd &lt;dbl&gt;, rec_var &lt;dbl&gt;, rec_mad &lt;dbl&gt; Which therapist has the highest attrition rate? Attrition couldn’t be calculated from the cleaned data, because we filtered out those observations! Since we have the number of people who stopped attending therapy and the number of people who continued to attend therapy, we can recover that information. Again, this could have been more easily solved by using the original clinician data. This data suggests that Ricardo has the highest attrition rate. attr_rate&lt;-clean_clinician%&gt;% group_by(Therapist)%&gt;% summarise(attr_rate = mean(attr_count)/(mean(attr_count)+n()))%&gt;% #Mean can be used because their is no variability within group clinicians. arrange(desc(attr_rate)) attr_rate ## # A tibble: 5 x 2 ## Therapist attr_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ricardo 0.254 ## 2 Blaine 0.244 ## 3 Nikola 0.232 ## 4 Samantha 0.228 ## 5 Dustin 0.227 Which therapist has the highest recovery rate? Because recovery rate is stored as a binary variable (i.e., 1 or 0), the proportion of patients recovered is equal to the average of the recovery column. We calculated this above. It suggests that Blaine has the highes recovery rate. grouped_stats%&gt;% select(Therapist, rec_m)%&gt;% arrange(desc(rec_m)) ## # A tibble: 5 x 2 ## Therapist rec_m ## &lt;chr&gt; &lt;dbl&gt; ## 1 Blaine 0.618 ## 2 Nikola 0.497 ## 3 Dustin 0.337 ## 4 Samantha 0.336 ## 5 Ricardo 0.316 Is there an association between the Recrutiment Medium and recovery rate? This was an advanced problem. There are several ways to approach this. I illustrate one method below. Using the cleaned data, I calculate the joint, conditional, and marginal probabilities. If the two variables are independent, the conditional probabilities should approximate the marginal probabilities (i.e., the probability of recovery across recruitment channels is equal to the probabilty of recovering overall) # Advanced Question: Calculate joint, conditional, and marginal probabilities ### Conditonal p should approximate marginal if independent clean_clinician%&gt;% count(Recruitment_Channel, recovery)%&gt;% # Uses count to count the joint frequency group_by(recovery)%&gt;% mutate(recovery_mf= sum(n))%&gt;% # Calculates the marginal frequencies of recovering and not recovering ungroup()%&gt;% group_by(Recruitment_Channel)%&gt;% mutate(recruitment_mf = sum(n))%&gt;% # Calculates the marginal frequencies of each recruitment channel ungroup()%&gt;% mutate(p = n/sum(n), # Converting joint frequencies to joint probabilities recovery_mp = recovery_mf/sum(n), # Converting marginal frequencies to marginal probabilities recruitment_mp = recruitment_mf/sum(n))%&gt;% group_by(Recruitment_Channel)%&gt;% mutate(recovery_cond_p = n/sum(n))%&gt;% # Calculate probability of recovery conditioned on channel select(Recruitment_Channel, recovery, p, recovery_mp, recovery_cond_p) # Selecting only probabilities ## # A tibble: 8 x 5 ## # Groups: Recruitment_Channel [4] ## Recruitment_Channel recovery p recovery_mp recovery_cond_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Email 0 0.156 0.582 0.578 ## 2 Email 1 0.114 0.418 0.422 ## 3 Friend Referral 0 0.134 0.582 0.557 ## 4 Friend Referral 1 0.107 0.418 0.443 ## 5 Google 0 0.154 0.582 0.639 ## 6 Google 1 0.0867 0.418 0.361 ## 7 Medical Referral 0 0.139 0.582 0.557 ## 8 Medical Referral 1 0.110 0.418 0.443 Follow-up Quesitons What was the difference between scale scores generated by arithmetic operators and functions? They handle missing data differently. mean() has the na.rm option that allows you to calculate an average using all available data. Did the information stored within group-level features differ from what was generated using grouped summary statistics? Nope! They are really generating the same information! The group-level feature extraction just repeats it across observations. When would it be useful to store group-level data as a summary table versus a variable in the original data frame? It is useful to store group-level data when you want to use it in a model. When you want to communicate data, it is more useful to store it in a separate table. Dplyr Chain with Some Advanced Functions # Cleaning Data ---------------------------------------------------------------------------------------------------- clean_clinician&lt;-clinician%&gt;% filter(attrition!=1)%&gt;% # Removing patients that no longer attend therapy (i.e., attrition == 1) select(-Pull_Date, -attrition)%&gt;% # Dropping variables with no variance (Pull_Date and attrition were the same across each obs) rename(beck1 = Q1.1, beck2 = Q1.2, beck3 = Q1.3, swb1 = Q2.1, swb2 = Q2.2, swb3 = Q2.3)%&gt;% # renaming to meaningful variables filter_at(.vars = vars(beck1, beck2, beck3, swb1, swb2, swb3), .vars_predicate = any_vars(!is.na(.)))%&gt;% # Removing observations if all are missing (retaining if responded on any observations) mutate(beck_arith = (beck1+beck2+beck3)/3, swb_arith = (swb1+swb2+swb3)/3)%&gt;% #Using arithmetic methods for means rowwise()%&gt;% # Creating scale scores with missing data mutate(beck_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE), swb_fun = mean(c(beck1, beck2, beck3), na.rm = TRUE))%&gt;% # Using function for means ungroup()%&gt;% # always making sure to ungroup mutate(dep_cdf = cume_dist(beck_fun)) # creating a new variable that contains the cdf for becks depression inventory # Extracting Group-level Features ------------------------------------------------------------------------------------- clean_clinician&lt;-clean_clinician%&gt;% # Using the cleaned data overwrite clean data. group_by(Therapist)%&gt;% # group the data by Therapist mutate(dep_cdf_grp = mean(beck_fun, na.rm = TRUE))%&gt;% # calculate the cdf by group ungroup() # Attrtion rate is no longer stored in this data. Below I show how to use join to add it to the cleaned data # A more efficient way would be to create the attr_count variable before I filtered (i.e., add_count(Therapist, attrition, name = &quot;attr_count)), but this is a good opportunity to using a join function attr_rate&lt;-clinician%&gt;% count(Therapist, attrition, name = &quot;attr_count&quot;) # Counting the number of patients that stopped comming to each clinician clean_clinician&lt;-left_join(clean_clinician, attr_rate) # Joining the new attrition count data with the cleaned data # Summary Tables ------------------------------------------------------------------------------------------------- ## summarise_all is useful, and depicted here, but summary() is equally acceptible! summary_stat&lt;-clean_clinician%&gt;% select(beck_fun, swb_fun, recovery)%&gt;% summarise_all(.funs = list(m = ~mean(., na.rm = TRUE), med = ~median(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE), var = ~var(., na.rm = TRUE), mad = ~mad(., na.rm = TRUE))) grouped_summary&lt;-clean_clinician%&gt;% select(Therapist, beck_fun, swb_fun, recovery)%&gt;% group_by(Therapist)%&gt;% summarise_all(.funs = list(m = ~mean(., na.rm = TRUE), med = ~median(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE), var = ~var(., na.rm = TRUE), mad = ~mad(., na.rm = TRUE))) # Printing both summaries summary_stat ## # A tibble: 1 x 15 ## beck_fun_m swb_fun_m recovery_m beck_fun_med swb_fun_med recovery_med ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.19 4.19 0.418 4.33 4.33 0 ## # ... with 9 more variables: beck_fun_sd &lt;dbl&gt;, swb_fun_sd &lt;dbl&gt;, ## # recovery_sd &lt;dbl&gt;, beck_fun_var &lt;dbl&gt;, swb_fun_var &lt;dbl&gt;, ## # recovery_var &lt;dbl&gt;, beck_fun_mad &lt;dbl&gt;, swb_fun_mad &lt;dbl&gt;, ## # recovery_mad &lt;dbl&gt; grouped_summary ## # A tibble: 5 x 16 ## Therapist beck_fun_m swb_fun_m recovery_m beck_fun_med swb_fun_med ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Blaine 3.97 3.97 0.618 4 4 ## 2 Dustin 4.21 4.21 0.337 4.33 4.33 ## 3 Nikola 4.29 4.29 0.497 4.33 4.33 ## 4 Ricardo 4.18 4.18 0.316 4.33 4.33 ## 5 Samantha 4.28 4.28 0.336 4.33 4.33 ## # ... with 10 more variables: recovery_med &lt;dbl&gt;, beck_fun_sd &lt;dbl&gt;, ## # swb_fun_sd &lt;dbl&gt;, recovery_sd &lt;dbl&gt;, beck_fun_var &lt;dbl&gt;, ## # swb_fun_var &lt;dbl&gt;, recovery_var &lt;dbl&gt;, beck_fun_mad &lt;dbl&gt;, ## # swb_fun_mad &lt;dbl&gt;, recovery_mad &lt;dbl&gt; # Attrition couldn&#39;t be calculated like recovery rate using the mean function because we filtered out those observations! #Since we have the number of people who stopped attending therapy and the number of people who continued to attend therapy, we can recover that information. # Again, this could have been more easily solved by using the original clinician information. # It had to be calculated separately attr_rate&lt;-clean_clinician%&gt;% group_by(Therapist)%&gt;% summarise(attr_rate = mean(attr_count)/(mean(attr_count)+n())) attr_rate ## # A tibble: 5 x 2 ## Therapist attr_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 Blaine 0.244 ## 2 Dustin 0.227 ## 3 Nikola 0.232 ## 4 Ricardo 0.254 ## 5 Samantha 0.228 # Based on this information Ricardo has the highest attrition rate attr_rate%&gt;% # Using attr_rate data select(Therapist, attr_rate)%&gt;% # select the Therapist and attr_rate columns arrange(desc(attr_rate)) # sore attr_rate column in descending order ## # A tibble: 5 x 2 ## Therapist attr_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ricardo 0.254 ## 2 Blaine 0.244 ## 3 Nikola 0.232 ## 4 Samantha 0.228 ## 5 Dustin 0.227 # Blaine has this highest recovery rate grouped_summary%&gt;% select(Therapist, recovery_m)%&gt;% arrange(desc(recovery_m)) ## # A tibble: 5 x 2 ## Therapist recovery_m ## &lt;chr&gt; &lt;dbl&gt; ## 1 Blaine 0.618 ## 2 Nikola 0.497 ## 3 Dustin 0.337 ## 4 Samantha 0.336 ## 5 Ricardo 0.316 # Relationship between Recruitment Channel and Recovery? ----------------------------------------------- clean_clinician%&gt;% count(Recruitment_Channel, recovery)%&gt;% # Uses count to count the joint frequency group_by(recovery)%&gt;% mutate(recovery_mf= sum(n))%&gt;% # Calculates the marginal frequencies of recovering and not recovering ungroup()%&gt;% group_by(Recruitment_Channel)%&gt;% mutate(recruitment_mf = sum(n))%&gt;% # Calculates the marginal frequencies of each recruitment channel ungroup()%&gt;% mutate(p = n/sum(n), # Converting joint frequencies to joint probabilities recovery_mp = recovery_mf/sum(n), # Converting marginal frequencies to marginal probabilities recruitment_mp = recruitment_mf/sum(n))%&gt;% group_by(Recruitment_Channel)%&gt;% mutate(recovery_cond_p = n/sum(n))%&gt;% # Calculate probability of recovery conditioned on channel select(Recruitment_Channel, recovery, p, recovery_mp, recovery_cond_p) # Selecting only probabilities ## # A tibble: 8 x 5 ## # Groups: Recruitment_Channel [4] ## Recruitment_Channel recovery p recovery_mp recovery_cond_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Email 0 0.156 0.582 0.578 ## 2 Email 1 0.114 0.418 0.422 ## 3 Friend Referral 0 0.134 0.582 0.557 ## 4 Friend Referral 1 0.107 0.418 0.443 ## 5 Google 0 0.154 0.582 0.639 ## 6 Google 1 0.0867 0.418 0.361 ## 7 Medical Referral 0 0.139 0.582 0.557 ## 8 Medical Referral 1 0.110 0.418 0.443 "],
["combining-data-sets.html", "Chapter 6 Combining Data Sets", " Chapter 6 Combining Data Sets Take a look the left panel of the tables above which shows the original survey data set after it was cleaned up. Manager information was not originally stored in this data! In order to get manager information into the data frame I did some magic behind the scenes. I merged the survey data set the manager data in the right pane based on the ResponseId variable. Luckily the creators of dplyr wrote a set of functions that make merging multiple data tables easy. In the following sections we are going to learn a variety of different ways to bind and join data frames. "],
["binding-functions.html", "Chapter 7 Binding Functions", " Chapter 7 Binding Functions Binding functions are the most basic method used to combine data sets in the tidyverse, although they are not appropriate for all cases. In the sections that follow we will review what these functions do and highlight cases in which they are and are not appropriate. Binding Functions bind_rows(): Stacks many data frames vertically. bind_cols(): Joins many date frames horizontally. "],
["bind-cols-binding-data-frames-horizontally.html", "7.1 bind_cols(): Binding Data Frames Horizontally", " 7.1 bind_cols(): Binding Data Frames Horizontally bind_cols() is used when you have a set of data frames that Have equal number of rows Are ordered identically, with no missing or new observations If these two requirements are not met, the data will joined combining information about different observations or participants If your data does not meet either of these requirements, but has a participant identifier use a join function discussed below. 7.1.1 bind_cols() Structure bind_cols(…) bind_cols() takes two or more data frames (or a list of data frames) that have an equal number of identically ordered rows. 7.1.2 Using bind_cols() Note that, while for the survey data we want to combine two data frames horizontally, the data frames do not have the same number of rows. Furthermore, based on the responseId variable, we know that participants are not in the same order. Thus we are probably better off using a different function to join these two data sets. In contrast, note that in the two tables below, each data frame has the same number of observations and the ID variables align perfectly. Figure 7.1: Two Measures with Shared Identifier Example 7.1 Binding data frames together horizontally. bind_cols(survey_data, perf_dat) Figure 7.2: Output of bind_cols Note that, since there is a duplicate column (ResponseId) in the new joined data set, col_bind() automatically added a 1 to the end of the column name. This is intended to prevent mix ups, but can result in duplicate data. While this example was adequate for illustration purposes, in practice, join functions are more flexible and appropriate when data sets have a shared identifier. "],
["bind-rows-binding-data-frames-vertically.html", "7.2 bind_rows(): Binding Data Frames Vertically", " 7.2 bind_rows(): Binding Data Frames Vertically bind_rows() is used when you want to bind data frames vertically. This is sometimes referred to as stacking data frames. Unlike bind_cols(), bind_rows() attempts to match columns based on their names. If a data frame is missing a column, observations will have missing data for that variable. 7.2.1 bind_rows() Structure bind_rows(…) bind_rows() takes two or more data frames (or a list of data frames) 7.2.2 Using bind_rows() Figure 7.3: Two Data Frames with Overlapping and Unique Information Example 7.2 Binding data frames together vertically (AKA, stacking). bind_rows(survey_data, perf_dat) Figure 7.4: Output of bind_rows() Note that the columns that are named the same are appropriately matched. Furthermore, observations from a data frames that is missing a column are assigned NA for that variable. "],
["mutating-joins.html", "Chapter 8 Mutating Joins", " Chapter 8 Mutating Joins Note that when we were using bind_cols(), corresponding rows in each data frame were assumed to belong to the same observation. This perfect match up rarely occurs unless data frames were programatically spit and manipulated by the user. In psychological research, participant attrition causes some observations to be present in one data set but not another. Furthermore, participants can respond in a different orders across time-points. bind_cols() may also be inadequate when merging data frames associated with different levels in a hierarchy (i.e., team and individual). When merging hierarchical data frames, if one team is associated with many individuals, team information may need to be repeated multiple times. The mutating join functions were developed with these problems in mind. Mutating join functions share a number of common characteristics. They share the same structural form. Data frames are horizontally combined so that the outputted data frame has more columns than either of the independent data frames. Rows are matched based on some common ID variable. If there are multiple rows with the same ID variable, all combinations of rows are returned. Despite their similarities, each mutating join differs in how it handles observations that do not match on an ID variable. In the sections that follow, I will: Define the common join function form. Described how each join function handles observations that do not have a match on the ID variable. Provide examples of uses for each form. List of Mutating Joins left_join(): Joins based on an ID variable. Retains all rows in left data frame and only matching rows in the right. right_join(): Joins based on an ID variable. Retains all rows in the right data frame and only matching rows in the left. inner_join(): Joins based on an ID variable. Retains only matching rows for both data frames. full_join(): Joins based on an ID variable (Considers order). Retains only matching rows for "],
["joinform.html", "8.1 Join Functions: Structural Form", " 8.1 Join Functions: Structural Form function(x, y, by = c(“lh_id” = “rh_id”)) function denotes the type of join you would like to perform (i.e., full_join, left_join). Join functions commonly refer to left-hand and right-hand and data frames. Whether a data frame is a left-hand or right-hand data frame is determined by what order you enter your the two data frames. x defines the left-hand data frame (it is the data frame argument furthest left). y defines the right-hand data frame (it is the data frame argument furthers right). by is an optional argument that defines the ID variables that will be used to match rows lh_id is the ID variable in the left-hand data frame while rh_id is the ID in the right-hand data frame If by is left NULL, join functions will search the for columns that share names and join those. "],
["full-join.html", "8.2 full_join()", " 8.2 full_join() full_join() retains all rows from left- and right-hand data frames. Figure 8.1: Two Data Frames with a Shared Identfier Example 8.1 Joining the survey data with managerial data, retaining all observations from both data frames. How are duplicate matches handled? full_join(survey_data, manager, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) Figure 8.2: Joining Data Frames using Full_join Example 8.2 Changing which data frame is the left-hand df and which data frame is the right-hand df changes the order of the columns but not which observations are kept. full_join(manager, survey_data, by = c(&quot;ResponseId&quot; = &quot;ResponseId&quot;)) Example 8.3 Since the data frames only share one variable with a commmon name. dplyr does give us a lovely message to let us know what the data frames are being joined by. This message will be suppressed from this point forward. full_join(manager, survey_data) ## Joining, by = &quot;ResponseId&quot; "],
["left.html", "8.3 left_join()", " 8.3 left_join() left_join() retains all observations in the left-hand data frame but only matching observations from the right-hand data frame. Example 8.4 Retaining all observations from the survey data, but only matching observations from the managerial data. left_join(survey_data, manager) Example 8.5 left_join() retains different observations depending on which data frame is in the left_hand position and which data frame is in the right-hand right hand position. left_join(manager, survey_data) "],
["right-join.html", "8.4 right_join()", " 8.4 right_join() Unsurprisingly, right_join() is the inverse of left_join(). It simply retains all observations in the right-hand data frame and only matching observations in the left-hand data frame Figure 8.3: Original Data Example 8.6 Note that this example produces output that is identical to Example 8.4. The only difference is the data frame arguments are flipped! right_join(manager, survey_data) "],
["inner-join.html", "8.5 inner_join()", " 8.5 inner_join() inner_join() only retains observations with matching IDs in both left-hand and right-hand data frames. For our example, there is one respondent that doesn’t have managerial information (ResponseId = R_2Sq8eFhNWEfZOJd). If the purpose of the survey was to provide managers with insight about their team, this person’s responses may not be useful. inner_join() can be used to exclude this person from subsequent reports. Figure 8.4: Original Data Example 8.7 inner_join() will exclude all respondents for whom we do not have managerial information and all employees who did not respond to the survey. inner_join(manager, survey_data) Figure 8.5: Only Retaining Observations that Match "],
["filtering-joins.html", "Chapter 9 Filtering Joins", " Chapter 9 Filtering Joins While mutating joins are useful, sometimes it is necessary to remove observations from a data frame based on information stored elsewhere without adding any information to a focal data frame. Filtering joins do just that. As their title suggests, filtering joins are kind of a hybrid between filter() and the join family of functions. They take the same structural form as mutating joins (discussed in Section 8.1) but remove or retain observations that do not correspond to any observations ID variable in the right-hand data frame. Filtering Joins semi_join(): Retains rows in the left hand data frame that match an ID variable in the right hand data frame. anti_join(): Retains rows in the left hand data frame that do NOT match and ID variable in the right hand data frame. "],
["semi-join.html", "9.1 semi_join()", " 9.1 semi_join() semi_join() retains observations in the left-hand data frame that have corresponding ID variables in the right-hand data frame. No new columns are added to the left-hand data frame. Figure 9.1: Original Data Example 9.1 Using semi_join() to retain observations for which we have managerial data without adding managerial data to the survey data. semi_join(survey_data, manager) "],
["anti-join.html", "9.2 anti_join()", " 9.2 anti_join() anti_join() retains observations in the left-hand data frame that do NOT have corresponding ID variables in the right-hand data frame. No new columns are added to the left-hand data frame. This can be especially useful when trying to identify cases that are not contained in one data frame, but stored in another. This can occur as a result of non-response, participant attrition, or random sampling (as we will see in the next chapter) Figure 9.2: Original Data Example 9.2 Using anti_join() to identify employees in the managerial data frame that have not yet responded to our survey. anti_join(manager, survey_data) Figure 9.3: Using anti_join() to identify employees that have yet to respond to the survey. "],
["activity-1.html", "Chapter 10 Activity", " Chapter 10 Activity Let’s see what you’ve learned! Relational databases are a common way to store data. Information associated with different levels is stored in separate tables. Tables are linked with foreign keys, a fancy name for ID variables. This organization reduces memory demands, but requires a strong knowledge of joins to extract piece together the information. IMBD stores their data about movies in a relational database format. You can find a description of the data files title IMDB.txt in the suppl folder, linked here. Again, instructions may be ambiguous. This is done intentionally to facilitate critical thinking when applying the principles learned above. Feel free to use other functions unless explictly told not to do so. Your goal is to identfiy the movies that have at least one “star” actor. In this excercise a “star” actor is considered to be an actor who’s rating average across all movies they have been in is in the top 10% of the rating distribution. To load the data please run the following code. # Loading tidyverse into memory library(tidyverse) # Reading csvs directly from github! ratings&lt;-read_csv(&quot;https://raw.githubusercontent.com/jimmyrigby94/Data-Management-in-R/master/suppl/ratings.csv&quot;) titles&lt;-read_csv(&quot;https://raw.githubusercontent.com/jimmyrigby94/Data-Management-in-R/master/suppl/titles.csv&quot;) principal_actors&lt;-read_csv(&quot;https://raw.githubusercontent.com/jimmyrigby94/Data-Management-in-R/master/suppl/principal_actors.csv&quot;) names&lt;-read_csv(&quot;https://raw.githubusercontent.com/jimmyrigby94/Data-Management-in-R/master/suppl/names.csv&quot;) Instructions Load the following .csv’s into your environment ratings.csv titles.csv principal_actors.csv names.csv Merge the principal_actors and names data frames retaining all observations from both objects to create a data frame titled cinematic_pros. Are we missing movie history for some cinematic professionals? Tests this by using an appropriate function call that identifies observations in names that don’t have a matching id variable in principal_actors. How many movie professionals do not have their acting history principal_actors? cinematc_pros contains information on more people than just actors. Create a new object called actors that only contains information about actor-movie combinations where they are explicitly categorized as actors. Merge the titles and ratings data sets retaining only matching observations in both dataframes to create a new dataframe called movies. Merge actors and movies using your favorite mutating join to create an objected titled full_data. Calculate summary statistics using the actors unique id. Store the below information in an object titled actor_summaries Count the number of movies each actor has been in. Calculate the average ratings for each actor. Create a cumulative distribution variable for the actors’ average ratings. Plot the average rating cumulative distribution. Using the actor_summaries and movies data, create a data frame that contains the movie information for the actors with ratings in the top 10%. "],
["other-functions-for-extracting-observations.html", "Chapter 11 Other Functions for Extracting Observations", " Chapter 11 Other Functions for Extracting Observations In the previous chapters, we learned several functions that can be used to extract observations from a data frame. filter() uses a logical test to extract observations. In contrast, filtering joins use an ID variable in another data frame to extract observations. In the chapters that follow we will cover functions that allow us to extract observations when we want to Want to take a random subset of observations Want to retain only distinct observations "],
["random-samples-of-observations.html", "11.1 Random Samples of Observations", " 11.1 Random Samples of Observations While very much beyond the scope of this lecture, randomly splitting a data set is a key operation for many statistical procedures. Researchers who want to cross-validated their models subset their data (sometimes \\(n\\) times) to evaluate its performance When a model’s statistical assumptions are violated, a researcher can implement a procedure called bootstrapping that uses resampling methods to estimate a parameters standard error. In short, knowing how to randomly sample a data set opens the door to many other statistical procedures The following sections will define the structural form, and show a few examples, however this section is not ended to go in depth into resampling methods. 11.1.1 Structural Form of sample_ Functions sample_x(data, size, replace, weight, …) sample_x denotes the sampling function you would like to use. data: specifies the data frame you would like to operate on. size: specifies the size of the sample you would like to take either in absolute or relative terms (depending on whether you use sample_n() or sample_frac()). replace: logical value specifying whether a data frame should be sampled with replacement (i.e., bootstrap). weight: a vector of weights equal to the number of observations in the data frame specifying how likely each observation is to be sampled (useful for stratified sampling). 11.1.2 Using sample_n() and sample_frac() sample_frac() and sample_n() only differ in terms of the size argument For sample_frac() you specify a proportion, relative to data argument. For sample_n() you specify an absolute number of rows for the output data. Arguably, sample_frac() is more robust to changes in upstream code. For example, if you catch a mistake in your data cleaning prior to taking a random sample of your data set, sample_frac() will still sample relative to this new data frame. In contrast, sample_n() does not adjust to changes in your code and will still resample based on the n you define. Putting differences aside, lets consider how a researcher could create a training and test data frame to evaluate their model’s performance. When randomly sampling the data frame, always make sure to use set.seed() so that the results are reproducible. I will also include and example that shows you how to work around the potential pitfalls of sample_n() by avoiding hard coding its size argument. Figure 11.1: Original Data Example 11.1 Using sample_n() and anti_join() to create training and test sets, hardcoding size. set.seed(123) training &lt;- sample_n(survey_data, size = 4, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.2: Using sample_n() to randomly sample data Example 11.2 Using sample_frac() and anti_join() to create training and test sets. set.seed(123) training &lt;- sample_frac(survey_data, size = .8, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.3: Using sample_frac to randomly sample data Example 11.3 Using sample_n() and anti_join() to create training and test sets, without hardcoding n. set.seed(123) rel_n&lt;- nrow(survey_data)*.8 training &lt;- sample_n(survey_data, size = rel_n, replace = FALSE) holdout &lt;- anti_join(survey_data, training) Figure 11.4: Avoiding Hard Coding n "],
["distinct-extracting-unique-observations.html", "11.2 distinct(): extracting unique observations", " 11.2 distinct(): extracting unique observations As we know, data collection methods are not perfect and neither are participants. Sometimes, software accidentally records duplicate observations. Furthermore, participants may take surveys more than once resulting in duplicate information for the same person. Identifying distinct responses is often of critical step in ensuring high fidelity data. The final observation extraction function we will cover provides a means of extracting unique cases. 11.2.1 distinct() Structure distinct(data, distinct_var, …, .keep_all) data: specifies the data frame you would like to operate on. distinct_var: defines a variable for which you would like to identify distinct levels. If multiple variables are provided, distinct identifies unique combinations of these levels. .keep_all: is a logical values that specifies whether or not to keep all other variables in the resulting output. Note that distinct() returns the first observations with a distinct level of distinct_var. This means that if .keep_all = TRUE is only really appropriate when an observation is a true duplicate (all information is redundant). 11.2.2 Using distinct() Figure 11.5: Original Data Let’s take a look at the manager data that we used when combining multiple data frames. Let’s use distinct() create data frames that: Store the names of each manager. Store the IDs associated with each employee. Store ID/manager combinations. Example 11.4 Using distinct() to extract unique levels of manager. manager%&gt;% distinct(Manager) Figure 11.6: Disinct Levels of Manager Example 11.5 Using distinct() to extract unique levels of ResponseId. manager%&gt;% distinct(Employee) Figure 11.7: Unique levels of ResponseId Example 11.6 Using distinct() to extract unique ResponseId-Manager combinations. manager%&gt;% distinct(Employee, Manager) Figure 11.8: Distinct ResponseId-Manager combinations "],
["performing-repeated-operations.html", "Chapter 12 Performing Repeated Operations", " Chapter 12 Performing Repeated Operations By this point, my hope is that you feel comfortable with the core dplyr functions. In the next chapters, we will discuss three variations on each of these functions that allow you to write one line of code to repeat a manipulation many times. While I will not demonstrated every variation of these functions, knowing they exist can help you speed up your data cleaning. What are some instances when you would want to do repeated manipulations? Renaming all all columns so that their names are lower case. Centering a set of independent variables for regression. Calculating summary statistics for a large set of variables. Converting a set of variables that were read as character to numeric. Rounding all numeric variables to the second decimal place. Most of the core functions have variations that variations that facilitate repeated operations in different ways. The names are mostly the same, except a suffix is added to the end to differentiate it from its typical call (i.e., mutate_all()) Each suffix defines the repeated manipulation in a different way. Summary of Suffix Definitions _all: Applies the transformation to all columns. _at: Applies the transformation to a set of columns you define. _if: Applies the transformation to a set of columns that match an argument. "],
["all-suffix.html", "12.1 all() suffix", " 12.1 all() suffix As you might suspect, functions with the _all() suffix apply your specified transformation to all columns in the data frame. This is useful when there is a single transformation that is appropriate for every column. For example, because R is case sensitive, it is much easier to always were with lower case column names. Using rename_all() will help us rename every single column so that it matches this pattern. To use rename_all() we simply define the data frame and then a function that will take a column name and change it in some way. tolower() converts string values to lowercase and is the appropriate function to use here. Figure 12.1: Original Data Example 12.1 Using rename_all() and tolower() to rename all variables so they are lower case. survey_data%&gt;% rename_all(tolower) Figure 12.2: Renamed All variables to lower case "],
["at-suffix.html", "12.2 at() suffix", " 12.2 at() suffix The at suffix applies a given transformation to a set of variables. It relies on the vars() helper function to define these variables. Let’s use the mutate_at() function to center the cons and perf columns In this case, I want to retain my centered and uncentered variables for later use. To do this, I defined a named list that contains the functions I want to apply to the variables I define. The names of the elements in the list will be appended to my original names to create new columns. .s are used as place holders for the vars. Figure 12.3: Original Data Example 12.2 Using mutate_at() to center cons and perf at 0. centered_data&lt;-survey_data%&gt;% mutate_at(vars(cons, perf), list(c = ~ .-mean(., na.rm = TRUE))) centered_data Figure 12.4: Centers Variables Defined in vars() Lets double check our work using summarise_at() Centering a variables changes it’s expected value but not its spread. This means that the new variables should have different means but identical standard deviations compared to the original variables. Example 12.3 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) Figure 12.5: Creates Summary Table for Variables Defined in vars() "],
["if-suffix.html", "12.3 if() suffix", " 12.3 if() suffix The _if suffix applies a transformation to a set of columns that satisfy a logical test. I have been using mutate_if() and round() behind the scenes while writing this book to format most of the tables that you see. Consider Example 12.3 when I don’t use mutate_if(). Example 12.4 The code you saw really generates this ugly beast. centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) Figure 12.6: Unrounded Data The floating point (maximum decimal point) in R goes out a long way making calculations very precise but output unwieldy. By using mutate_if(), we can apply round() repeatedly across the data frame. round() only works with numeric data, though, so we want to avoid applying it character and factor data. Example 12.5 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE)))%&gt;% mutate_if(is.numeric, round, digits = 5) Figure 12.7: Formatting Output using mutate_if "],
["wider-and-longer-data-formats.html", "Chapter 13 Wider and Longer Data Formats", " Chapter 13 Wider and Longer Data Formats You now know how to efficiently add/remove columns, remove rows, and summarise information contained within a data frame. There are a few more tools you need to learn before you can handle most data management tasks. You have seen one form longitudinal data can take in Chapter 1 (separate objects). It can also come in long format and wide format, depicted below. Transitioning back and forth between these formats is an important skill to have, because different analyses require different data formats. For example, most multi-level modeling software take the data in long format. In contrast, many MANCOVA packages require wide format data. Luckily, tidyr, another package written by Hadley Wickham, is specially designed to reshape data. tidyr contains a set of functions for wrangling messy data and making it tidy. tidy data has the following properties. Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. Please see the tidyr vignette for more information on tidy data. Core tidyr Functions for Reshaping Data gather(): Make a data frame longer spread(): Make a data frame wider Figure 13.1: Wide Format Figure 13.2: Long Format "],
["gather-wider-to-longer.html", "13.1 gather(): Wider to Longer", " 13.1 gather(): Wider to Longer gather() is a function intended to take wide format data and make it longer. It does this by gathering a set of columns in the wide data into one column. The user defines the columns columns that should be collapsed, the name of the column that will store the original columns’ names, and the name of the column that will store the original columns’ values. 13.1.1 gather() Structure gather(data, key, value, …) key: Name of column to store wide format column names. value: Name of column to store the selected columns values. …: A selection of columns to gather into long format. This operates similar to select() and can accommodate special operators like :, -, starts_with, etc. 13.1.2 Using gather() gather() is useful for converting a data frame into a longer format. We will illustrate this with the wide data set shown previously This data frame has to columns for a test core taken at two different time points These are labeled “pre” and “post” gather() can be used to stack the pre and post columns into a single test column which we will define with the value The temporal information can be stored in a separate column which we define with the key argument Figure 13.3: Wide Format Data Example 13.1 We can specify the columns we want to gather using the elipse argument gather(wide, key = &quot;time&quot;, value = &quot;test&quot;, pre, post) Figure 13.4: Using gather() to Go From Wide to Long Example 13.2 Equivalent sytax would be to use - to deselect columns to be gathered. In some use cases, this is quicker. gather(wide, key = &quot;time&quot;, value = &quot;test&quot;, -c(id, gender, condition, non_naieve)) "],
["spread-longer-to-wider.html", "13.2 spread(): Longer to Wider", " 13.2 spread(): Longer to Wider spread() is gather()’s conjugate. It converts long data frames into wider data frames. To do this, the user specifies a key column, that stores variable names, and a value column that stores the information associated with those variables. The key column is spread so that each unique value stored within it becomes a new column storing the associated values. 13.2.1 spread() Structure spread(data, key, value) key: Name of column to store wide format column names. value: Name of column to store the selected columns values. 13.2.2 Using spread() Figure 13.5: Long Data Example 13.3 spread(wide, key = &quot;time&quot;, value = &quot;test&quot;) Figure 13.6: Using Spread to Go From Long to Wide "],
["recent-developments.html", "13.3 Recent Developments", " 13.3 Recent Developments The tidyverse is under constant development by a team of very smart people. Recently, Hadley Wickham, one of the key contributes to the tidyverse announced new functions that will come to replace gather() and spread. To prepare you for this eventuality, I am going to introduce pivot_wider() and pivot_longer(). Given that they are still in development, I will only cover the very basics of these functions. They are only available in the tidyr development version and are not yet available in the tidyverse package To download the development version restart your R session (ctr+shift+f10) and run the following code install.packages(&quot;devtools&quot;) devtools::install_github(&quot;tidyverse/tidyr&quot;) 13.3.1 The Problems with gather() and spread() Gather and spread will throw errors when working with some data types. Some find the functions non-intuitive. The latest iteration of these functions make them more robust and also more intuitive. Cannot extract meta-data from column names. "],
["pivot-longer-gathers-predecessor.html", "13.4 pivot_longer(): gather’s() predecessor", " 13.4 pivot_longer(): gather’s() predecessor pivot_longer() is the most recent iteration of gather() It takes a data set and transforms it so it is longer, just like gather(). 13.4.1 pivot_longer() structure pivot_longer(data, names_to, values_to, cols, …) cols: The columns to gather, defined similar to select() and gather() names_to: Column name to store the former column names. values_to: Column name to store the former value names. …: to see additional arguments run ?pivot_longer. 13.4.2 Using pivot_longer() Figure 13.7: Wide Data Example 13.4 We can specify the columns we want to gather using the elipse argument pivot_longer(wide, cols = c(pre, post), key = &quot;time&quot;, value = &quot;test&quot;) Figure 13.8: Using pivot_longer() to go from Wide to Long "],
["pivot-wider-spreads-predecessor.html", "13.5 pivot_wider(): spread()’s predecessor", " 13.5 pivot_wider(): spread()’s predecessor pivot_wider() is the most recent iteration of spread(). pivot_wider() takes a data frame and makes it wider. 13.5.1 pivot_wider() Structure pivot_wider(data, names_from, values_from, …) names_from: Column name to store the former column names. values_from: Column name to store the former value names. …: to see additional arguments run ?pivot_longer. Figure 13.9: Long Data Example 13.3 pivot_wider(long, names_from = &quot;time&quot;, values_from = &quot;test&quot;) Figure 13.10: Using pivot_wider() to Go From Long to Wide "],
["formatted-summary-statistics.html", "Chapter 14 Formatted Summary Statistics", " Chapter 14 Formatted Summary Statistics You now have the tools for efficiently managing data in one or multiple objects! You may be wondering “What do I do with my new found skills?”. In the chapter that follows, I will illustrate how dplyr and tidyr can be harnessed to quickly complete a foundational task for any professional working with data: calculate summary statistics. "],
["a-quick-review-necessary-base-functions.html", "14.1 A Quick Review: Necessary Base Functions", " 14.1 A Quick Review: Necessary Base Functions There are a few key base R functions that are necessary for you to know by heart. These are listed below along with their arguments and defaults. If you have questions, remember you can use ?function_name to open the help file. Central Tendency Functions - `mean(x, trim = 0, na.rm = FALSE)` - `median(x, na.rm = FALSE)` Spread Functions - `var(x, na.rm = FALSE)` - `sd(x, na.rm = FALSE)` Measures of Association (A preview of things to come) - `cor(x, y = NULL, use = &quot;everything&quot;, method = &quot;peason&quot;)` - `cov(x, y = NULL, use = &quot;everything&quot;, method = &quot;peason&quot;)` "],
["beyond-base-r.html", "14.2 Beyond Base R", " 14.2 Beyond Base R There are a few functions that I have found especially helpful over the years. The standard base R functions do not provide hypothesis tests along with measures of association. The correlation function in the psycho package provides this and more! Table formatting also takes a ton of time! The apaTables function stream lines this process install.packages(&quot;psycho&quot;) install.packages(&quot;apaTables&quot;) install.packages(&quot;skimr&quot;) Measures of Association - psycho::correlation(df, type = \"full\", method = \"peason\", adjust = \"holm\", i_am_cheating = FALSE) Unformatted Summary Statistics -skimr::skim(data) Formatted Summary Tables - apaTables::apa.cor.table(data, filename = NA, table.number = NA, showconf.interval = TRUE, landscape = TRUE) "],
["prepping-environment.html", "14.3 Prepping Environment", " 14.3 Prepping Environment As always, it is necessary to load packages into the environment. This makes the functions within tha pacakges callable without some additional voodoo. This code chunk, I also load the Chaterjee-Price Attitude Data. This is an employee survey of clerical employees in a financial organization. Type ?attitudeto learn more. # Loading required packages library(psycho) library(skimr) library(kableExtra) library(apaTables) library(tidyverse) # Loading Data from built-in data set attitude_data&lt;-attitude "],
["inspecting-data.html", "14.4 Inspecting Data", " 14.4 Inspecting Data Prior to diving into the data, lets inspect it to see what we are dealing with. # Printing an overview of the data glimpse(attitude_data) ## Observations: 30 ## Variables: 7 ## $ rating &lt;dbl&gt; 43, 63, 71, 61, 81, 43, 58, 71, 72, 67, 64, 67, 69,... ## $ complaints &lt;dbl&gt; 51, 64, 70, 63, 78, 55, 67, 75, 82, 61, 53, 60, 62,... ## $ privileges &lt;dbl&gt; 30, 51, 68, 45, 56, 49, 42, 50, 72, 45, 53, 47, 57,... ## $ learning &lt;dbl&gt; 39, 54, 69, 47, 66, 44, 56, 55, 67, 47, 58, 39, 42,... ## $ raises &lt;dbl&gt; 61, 63, 76, 54, 71, 54, 66, 70, 71, 62, 58, 59, 55,... ## $ critical &lt;dbl&gt; 92, 73, 86, 84, 83, 49, 68, 66, 83, 80, 67, 74, 63,... ## $ advance &lt;dbl&gt; 45, 47, 48, 35, 47, 34, 35, 41, 31, 41, 34, 41, 25,... # Verifying that the double type is actually numeric mode(attitude_data$rating) ## [1] &quot;numeric&quot; It looks like we are dealing with 30 observations of 7 variables. Each of these variables is a double (meaning they have a numeric mode). Based on this information, it is safe to say that we can treat this data as numeric and calculate summary statistics reporting the central tendency, spread, and association between these variables. "],
["quick-and-dirty-mean-and-central-tendency.html", "14.5 Quick and Dirty Mean and Central Tendency", " 14.5 Quick and Dirty Mean and Central Tendency 14.5.1 skim() for Basic Summary Statistics When working with data initially, it is useful to calculate a wide variety of summary statistics. This provides the scientist with an idea about how their data looks, what its scale is, and how interrelated variables are. skim() in the skimr package calculates a range of summary statistics and histograms! It even treats categorical variables differently, if you have them, returning frequencies and levels. All you have to do is pass it a data frame! skim(attitude_data) ## Skim summary statistics ## n obs: 30 ## n variables: 7 ## ## -- Variable type:numeric --------------------------------------------------- ## variable missing complete n mean sd p0 p25 p50 p75 p100 ## advance 0 30 30 42.93 10.29 25 35 41 47.75 72 ## complaints 0 30 30 66.6 13.31 37 58.5 65 77 90 ## critical 0 30 30 74.77 9.89 49 69.25 77.5 80 92 ## learning 0 30 30 56.37 11.74 34 47 56.5 66.75 75 ## privileges 0 30 30 53.13 12.24 30 45 51.5 62.5 83 ## raises 0 30 30 64.63 10.4 43 58.25 63.5 71 88 ## rating 0 30 30 64.63 12.17 40 58.75 65.5 71.75 85 ## hist ## &lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt; ## &lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2586&gt;&lt;U+2583&gt; ## &lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt; ## &lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt; ## &lt;U+2582&gt;&lt;U+2583&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2582&gt;&lt;U+2581&gt; ## &lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2581&gt; ## &lt;U+2582&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2582&gt;&lt;U+2583&gt; 14.5.2 summarise() for Custom Summary Statistics skim() is very useful, but doesn’t give all the information you may need! Notice that skim() doesn’t produce the variance of the objects. While that is easy to get to from sd, we can use summarise() to get this information directly from the data. attitude_data%&gt;% summarise( advance_var = var(advance), complaints_var = var(complaints), privileges_var = var(privileges), learning_var = var(learning), raises_var = var(raises), critical_var = var(critical)) ## advance_var complaints_var privileges_var learning_var raises_var ## 1 105.8575 177.2828 149.7057 137.7575 108.1023 ## critical_var ## 1 97.9092 That was a lot of typeing the same basic thing, over and over! Remember summarise_if()? We can use summarise_if() along with the appropriate predicate function to calculate the variance of all numeric variables! Since our goal is to summarise the numeric variables our predicate will be is.numeric. attitude_data%&gt;% summarise_if(.predicate = is.numeric, .funs = list(var = var)) ## rating_var complaints_var privileges_var learning_var raises_var ## 1 148.1713 177.2828 149.7057 137.7575 108.1023 ## critical_var advance_var ## 1 97.9092 105.8575 "],
["quick-and-dirty-measures-of-association.html", "14.6 Quick and Dirty Measures of Association", " 14.6 Quick and Dirty Measures of Association The above functions are great for summarising the central tendency and spread of variables in a data, but we have gained little insight into how the varaibles are associated. cov() and cor() return variance-covariance and correlation matrices respectively. These scales provide an indice of association between variables. More positive numbers suggest a stronger positive association and more negative numbers suggest a stronger negative association. Note that differences in scale (i.e., spread) makes different covariances difficult to compare. Correlations are on a standard scale and give more insight into the relative proportion of variance explained. # Covariance cov(attitude_data) ## rating complaints privileges learning raises critical ## rating 148.17126 133.77931 63.46437 89.10460 74.68851 18.84253 ## complaints 133.77931 177.28276 90.95172 93.25517 92.64138 24.73103 ## privileges 63.46437 90.95172 149.70575 70.84598 56.67126 17.82529 ## learning 89.10460 93.25517 70.84598 137.75747 78.13908 13.46782 ## raises 74.68851 92.64138 56.67126 78.13908 108.10230 38.77356 ## critical 18.84253 24.73103 17.82529 13.46782 38.77356 97.90920 ## advance 19.42299 30.76552 43.21609 64.19770 61.42299 28.84598 ## advance ## rating 19.42299 ## complaints 30.76552 ## privileges 43.21609 ## learning 64.19770 ## raises 61.42299 ## critical 28.84598 ## advance 105.85747 # Correlation cor(attitude_data) ## rating complaints privileges learning raises critical ## rating 1.0000000 0.8254176 0.4261169 0.6236782 0.5901390 0.1564392 ## complaints 0.8254176 1.0000000 0.5582882 0.5967358 0.6691975 0.1877143 ## privileges 0.4261169 0.5582882 1.0000000 0.4933310 0.4454779 0.1472331 ## learning 0.6236782 0.5967358 0.4933310 1.0000000 0.6403144 0.1159652 ## raises 0.5901390 0.6691975 0.4454779 0.6403144 1.0000000 0.3768830 ## critical 0.1564392 0.1877143 0.1472331 0.1159652 0.3768830 1.0000000 ## advance 0.1550863 0.2245796 0.3432934 0.5316198 0.5741862 0.2833432 ## advance ## rating 0.1550863 ## complaints 0.2245796 ## privileges 0.3432934 ## learning 0.5316198 ## raises 0.5741862 ## critical 0.2833432 ## advance 1.0000000 14.6.1 Measures of Association With Hypothesis Tests While the base R functions are nice, they do not provide any indication about the confidence in our estimate. correlation() in the psycho package provides a verbal interpretation along with signicance tests which have been corrected for multiple comparisons. # Using the correlation function in psycho my_correlation&lt;-correlation(attitude_data) # Printing the textual interpretation my_correlation$text ## [1] &quot;Pearson Full correlation (p value correction: holm):\\n&quot; ## [2] &quot; - rating / complaints: Results of the Pearson correlation showed a significant large, and positive association between rating and complaints (r(28) = 0.83, p &lt; .001***).&quot; ## [3] &quot; - rating / privileges: Results of the Pearson correlation showed a non significant moderate, and positive association between rating and privileges (r(28) = 0.43, p &gt; .1).&quot; ## [4] &quot; - complaints / privileges: Results of the Pearson correlation showed a significant large, and positive association between complaints and privileges (r(28) = 0.56, p &lt; .05*).&quot; ## [5] &quot; - rating / learning: Results of the Pearson correlation showed a significant large, and positive association between rating and learning (r(28) = 0.62, p &lt; .01**).&quot; ## [6] &quot; - complaints / learning: Results of the Pearson correlation showed a significant large, and positive association between complaints and learning (r(28) = 0.60, p &lt; .01**).&quot; ## [7] &quot; - privileges / learning: Results of the Pearson correlation showed a non significant moderate, and positive association between privileges and learning (r(28) = 0.49, p = 0.07°).&quot; ## [8] &quot; - rating / raises: Results of the Pearson correlation showed a significant large, and positive association between rating and raises (r(28) = 0.59, p &lt; .01**).&quot; ## [9] &quot; - complaints / raises: Results of the Pearson correlation showed a significant large, and positive association between complaints and raises (r(28) = 0.67, p &lt; .01**).&quot; ## [10] &quot; - privileges / raises: Results of the Pearson correlation showed a non significant moderate, and positive association between privileges and raises (r(28) = 0.45, p &gt; .1).&quot; ## [11] &quot; - learning / raises: Results of the Pearson correlation showed a significant large, and positive association between learning and raises (r(28) = 0.64, p &lt; .01**).&quot; ## [12] &quot; - rating / critical: Results of the Pearson correlation showed a non significant small, and positive association between rating and critical (r(28) = 0.16, p &gt; .1).&quot; ## [13] &quot; - complaints / critical: Results of the Pearson correlation showed a non significant small, and positive association between complaints and critical (r(28) = 0.19, p &gt; .1).&quot; ## [14] &quot; - privileges / critical: Results of the Pearson correlation showed a non significant small, and positive association between privileges and critical (r(28) = 0.15, p &gt; .1).&quot; ## [15] &quot; - learning / critical: Results of the Pearson correlation showed a non significant small, and positive association between learning and critical (r(28) = 0.12, p &gt; .1).&quot; ## [16] &quot; - raises / critical: Results of the Pearson correlation showed a non significant moderate, and positive association between raises and critical (r(28) = 0.38, p &gt; .1).&quot; ## [17] &quot; - rating / advance: Results of the Pearson correlation showed a non significant small, and positive association between rating and advance (r(28) = 0.16, p &gt; .1).&quot; ## [18] &quot; - complaints / advance: Results of the Pearson correlation showed a non significant small, and positive association between complaints and advance (r(28) = 0.22, p &gt; .1).&quot; ## [19] &quot; - privileges / advance: Results of the Pearson correlation showed a non significant moderate, and positive association between privileges and advance (r(28) = 0.34, p &gt; .1).&quot; ## [20] &quot; - learning / advance: Results of the Pearson correlation showed a significant large, and positive association between learning and advance (r(28) = 0.53, p &lt; .05*).&quot; ## [21] &quot; - raises / advance: Results of the Pearson correlation showed a significant large, and positive association between raises and advance (r(28) = 0.57, p &lt; .05*).&quot; ## [22] &quot; - critical / advance: Results of the Pearson correlation showed a non significant small, and positive association between critical and advance (r(28) = 0.28, p &gt; .1).&quot; # Inspecting the correlelogram my_correlation$plot # Printing the correlation table with significance values my_correlation$summary ## rating complaints privileges learning raises critical ## rating ## complaints 0.83*** ## privileges 0.43 0.56* ## learning 0.62** 0.6** 0.49 ## raises 0.59** 0.67** 0.45 0.64** ## critical 0.16 0.19 0.15 0.12 0.38 ## advance 0.16 0.22 0.34 0.53* 0.57* 0.28 14.6.2 Formatted APA Summary Statistics Reporting summary statistics typically requires a bit more work to be put into formatting. However, apa.cor.table() really makes the job as easy as possible. The only thing we really have to do is ensure that are variables are properly named and capitalized before calling the function. This is because they will be used directly in the generated table. For the attitude_data, we just need to capitlize the variables. I use rename_all() to do this quickly attitude_data%&gt;% rename_all(.funs = ~str_to_sentence(.))%&gt;% apa.cor.table(filename = &quot;APA_Attitude_Table.doc&quot;, table.number = 1) ## ## ## Table 1 ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 ## 1. Rating 64.63 12.17 ## ## 2. Complaints 66.60 13.31 .83** ## [.66, .91] ## ## 3. Privileges 53.13 12.24 .43* .56** ## [.08, .68] [.25, .76] ## ## 4. Learning 56.37 11.74 .62** .60** .49** ## [.34, .80] [.30, .79] [.16, .72] ## ## 5. Raises 64.63 10.40 .59** .67** .45* ## [.29, .78] [.41, .83] [.10, .69] ## ## 6. Critical 74.77 9.89 .16 .19 .15 ## [-.22, .49] [-.19, .51] [-.22, .48] ## ## 7. Advance 42.93 10.29 .16 .22 .34 ## [-.22, .49] [-.15, .54] [-.02, .63] ## ## 4 5 6 ## ## ## ## ## ## ## ## ## ## ## ## .64** ## [.36, .81] ## ## .12 .38* ## [-.25, .46] [.02, .65] ## ## .53** .57** .28 ## [.21, .75] [.27, .77] [-.09, .58] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## "],
["the-pygmalion-effect-self-efficacy-based-intervention.html", "Chapter 15 The Pygmalion Effect: Self-efficacy based intervention", " Chapter 15 The Pygmalion Effect: Self-efficacy based intervention You are conducting a study on self-efficacy based interventions on short term memory. Before diving into your studies focal analysis, you want to dig into the data a bit more! Use dplyr and experimental_data to answer the following questions. Note that experimental_data is created in the first R chunk. Do not overwrite the original data for each question. 15.0.1 Data Creation (Do Not Change) set.seed(783623) library(tidyverse) experimental_data&lt;-tibble(participant_id = runif(120, min = 10000000, max = 99999999), session = sample(factor(x = c(&quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;), levels = c(&quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;), ordered = TRUE), size = 120, replace = TRUE, prob = c(.25, .30, .45)), Administrator = sample(c(&quot;UG&quot;, &quot;Graduate&quot;), size = 120, c(.5, .5), replace = TRUE))%&gt;% mutate(intervention_finished = case_when(session == &quot;morning&quot; ~ rbinom(n(), 1, .75), session == &quot;midday&quot; ~ rbinom(n(), 1, .80), session == &quot;evening&quot; ~ rbinom(n(), 1, .5)))%&gt;% mutate(memory_task_pre = sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.15, .2, .15, .2, .3)), memory_task_post = case_when(intervention_finished == 1 ~ sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.3, .4, .2, .05, .05)), intervention_finished == 0 ~ sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.2, .2, .3, .1, .2)))) "],
["data-manipulation.html", "15.1 Data Manipulation", " 15.1 Data Manipulation 15.1.1 Question 1 Completely deidentify the data by removing participant ids. Identify two ways to do this using the appropriate dplyr function. What is the first column in this new data? 15.1.2 Question 2 Create a data frame that only contains participants that have finished the intervention. How many participants completed the intervention? 15.1.3 Question 3 Using the original data frame, create a new data frame that stores observations of people who did not complete the intervention and performed terribly on the pre test. How many participants meet this criteria? 15.1.4 Question 4 Using the original data, reate a new variable that contains the number of students in each session. How many students were in the 17th student’s session? 15.1.5 Question 5 Create a new variable that contains a 1 if the student is the student was in the midday session and a 0 otherwise. What is the sum of that column? "],
["probability-with-dplyr.html", "15.2 Probability with dplyr", " 15.2 Probability with dplyr 15.2.1 Question 6 Count (hint hint) the number of people who completed and failed to complete your intervention. How many people completed the study. 15.2.2 Question 7 Add a column to freq_table that stores the proportion of participants that completed/failed to complete your intervention. What proportion of people failed to complete the study? 15.2.3 Question 8 Create a summary (hint hint) table that contains the joint frequencies of the memory task post test and the intervention completion. How many people had an excellent post test score AND completed the intervention? 15.2.4 Question 9 Using the summary table created in Question 8, covert the joint frequencies into joint probabilities. What is the probability of not completing the intervention and doing adequate on the post test. 15.2.5 Question 10 Using the summary table created in Question 9, calculate the marginal probabilities for each level of the memory test scores (advanced). "],
["solutions-to-the-pygmalion-effect-self-efficacy-based-intervention.html", "Chapter 16 Solutions to The Pygmalion Effect: Self-efficacy based intervention", " Chapter 16 Solutions to The Pygmalion Effect: Self-efficacy based intervention You are conducting a study on self-efficacy based interventions on short term memory. Before diving into your studies focal analysis, you want to dig into the data a bit more! Use dplyr and experimental_data to answer the following questions. Note that experimental_data is created in the first R chunk. Do not overwrite the original data for each question. 16.0.1 Data Creation (Do Not Change) set.seed(783623) library(tidyverse) experimental_data&lt;-tibble(participant_id = runif(120, min = 10000000, max = 99999999), session = sample(factor(x = c(&quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;), levels = c(&quot;morning&quot;, &quot;midday&quot;, &quot;evening&quot;), ordered = TRUE), size = 120, replace = TRUE, prob = c(.25, .30, .45)), Administrator = sample(c(&quot;UG&quot;, &quot;Graduate&quot;), size = 120, c(.5, .5), replace = TRUE))%&gt;% mutate(intervention_finished = case_when(session == &quot;morning&quot; ~ rbinom(n(), 1, .75), session == &quot;midday&quot; ~ rbinom(n(), 1, .80), session == &quot;evening&quot; ~ rbinom(n(), 1, .5)))%&gt;% mutate(memory_task_pre = sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.15, .2, .15, .2, .3)), memory_task_post = case_when(intervention_finished == 1 ~ sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.3, .4, .2, .05, .05)), intervention_finished == 0 ~ sample(size = n(), c(&quot;Excellent&quot;, &quot;Good&quot;, &quot;Adequate&quot;, &quot;Poor&quot;, &quot;Terrible&quot;), replace = TRUE, prob = c(.2, .2, .3, .1, .2)))) "],
["data-manipulation-1.html", "16.1 Data Manipulation", " 16.1 Data Manipulation 16.1.1 Question 1 Completely deidentify the data by removing participant ids. Identify two ways to do this using the appropriate dplyr function. What is the first column in this new data? # I use select and the minus sign to drop the participant id column from the experimental data Q1.dat&lt;-experimental_data%&gt;% select(-participant_id) # I then use colnames and the square brackets to index the column names in Q1.dat. You can also just print the data frame. colnames(Q1.dat)[1] ## [1] &quot;session&quot; 16.1.2 Question 2 Create a data frame that only contains participants that have finished the intervention. How many participants completed the intervention? # I use filter to return observations from experimental data that successfully completed the intervention. Q2.dat&lt;-experimental_data%&gt;% filter(intervention_finished == 1) # Since filter only returns the observations that passed a logical argument, the number of rows in the output tells us the number of participants that completed the intervention. nrow(Q2.dat) ## [1] 79 16.1.3 Question 3 Using the original data frame, create a new data frame that stores observations of people who did not complete the intervention and performed terribly on the pre test. How many participants meet this criteria? # Again, I use filter to return observations that meet this compound logical test. filter(intervention_finished != 1, memory_task_pre == &quot;Terrible&quot;) would return the same results. The ampersand just makes the logical test explicit. Q3.dat&lt;-experimental_data%&gt;% filter(intervention_finished != 1 &amp; memory_task_pre == &quot;Terrible&quot;) # nrow again returns the number of participants that satisfy the logical test. nrow(Q3.dat) ## [1] 14 16.1.4 Question 4 Using the original data, reate a new variable that contains the number of students in each session. How many students were in the 17th student’s session? # One way to add a count variable is to use add_count() Q2.dat.1&lt;-experimental_data%&gt;% add_count(session) # We can also use mutate() and group_by() Q2.dat.2&lt;- experimental_data%&gt;% group_by(session)%&gt;% mutate(n = n())%&gt;% ungroup() # don&#39;t forget your ungroup() # I then index the column n within Q2.dat.2$n using square brackets Q2.dat.2$n[17] ## [1] 40 # A logical test suggests both add_count and group_by-mutate() get the job done identical(Q2.dat.2$n, Q2.dat.1$n) ## [1] TRUE 16.1.5 Question 5 Create a new variable that contains a 1 if the student is the student was in the midday session and a 0 otherwise. What is the sum of that column? # This problem requires conditional processing # I use if_else within mutate. if_else returns one value if a logical test evaluates as true and another if it evaluates as false. Q5.1&lt;-experimental_data%&gt;% mutate(midday = if_else(session == &quot;midday&quot;, 1, 0)) sum(Q5.1$midday) ## [1] 40 "],
["probability-with-dplyr-1.html", "16.2 Probability with dplyr", " 16.2 Probability with dplyr 16.2.1 Question 6 Count (hint hint) the number of people who completed and failed to complete your intervention. How many people completed the study. # The count function can be used to create frequency tables freq_table&lt;-experimental_data%&gt;% count(intervention_finished) # Heres a look at the table freq_table ## # A tibble: 2 x 2 ## intervention_finished n ## &lt;int&gt; &lt;int&gt; ## 1 0 41 ## 2 1 79 # Looking at the table, it is clear that 79 people completed the intervention. We can us indexing to pull that number out incase we want to use it in the future freq_table[freq_table$intervention_finished==1, &quot;n&quot;] ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 79 16.2.2 Question 7 Add a column to freq_table that stores the proportion of participants that completed/failed to complete your intervention. What proportion of people failed to complete the study? # Working with frequencies in dplyr is nice becuase we can use core dplyr function to manipulate our tables # mutate() can be used to add the column of probabilities to the table p_freq&lt;-freq_table%&gt;% mutate(p = n/sum(n)) # Here I index the column p and the row that is associated with not completing the intervention p_freq[p_freq$intervention_finished==0, &quot;p&quot;] ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 0.342 16.2.3 Question 8 Create a summary (hint hint) table that contains the joint frequencies of the memory task post test and the intervention completion. How many people had an excellent post test score AND completed the intervention? # Like the hint suggests, we can use group_by() and summarise() to get joint frequencies across to variables jf.1&lt;-experimental_data%&gt;% group_by(memory_task_post, intervention_finished)%&gt;% summarise(n = n())%&gt;% ungroup() jf.1 ## # A tibble: 10 x 3 ## memory_task_post intervention_finished n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Adequate 0 9 ## 2 Adequate 1 14 ## 3 Excellent 0 10 ## 4 Excellent 1 17 ## 5 Good 0 9 ## 6 Good 1 40 ## 7 Poor 0 6 ## 8 Poor 1 5 ## 9 Terrible 0 7 ## 10 Terrible 1 3 # Count can be used in this case too (I just wanted to get you practice with summarise()). jf.2&lt;-experimental_data%&gt;% count(memory_task_post, intervention_finished) jf.2 ## # A tibble: 10 x 3 ## memory_task_post intervention_finished n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Adequate 0 9 ## 2 Adequate 1 14 ## 3 Excellent 0 10 ## 4 Excellent 1 17 ## 5 Good 0 9 ## 6 Good 1 40 ## 7 Poor 0 6 ## 8 Poor 1 5 ## 9 Terrible 0 7 ## 10 Terrible 1 3 # I use filter to and the square brackets to pull out n. This is an advanced technique and regular indexing is perfectly accepible jf.1%&gt;% filter(memory_task_post==&quot;Excellent&quot;, intervention_finished==1)%&gt;% .[&quot;n&quot;] ### Note that the period acts as a place holder for the data set when working with the pipe function. ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 17 16.2.4 Question 9 Using the summary table created in Question 8, covert the joint frequencies into joint probabilities. What is the probability of not completing the intervention and doing adequate on the post test. # This is done simply by deviding the frequencies calculated above by the total observations. # Take a minute to make sure your probabilities are summing to 0. If not did you forget to call ungroup() above (I did the first time!). # If you did forget to call ungroup(), congratulations, you accidently computed conditional probabilities. jf_p&lt;-jf.1%&gt;% mutate(p = n/sum(n)) jf_p$p[jf_p$intervention_finished==0 &amp; jf_p$memory_task_post==&quot;Adequate&quot;] ## [1] 0.075 16.2.5 Question 10 Using the summary table created in Question 9, calculate the marginal probabilities for each level of the memory test scores (advanced). # Remember, marginal proabilities sum across all joint probabilities at a given level of x. # This is easily done grouping by the focal variable, and summing across all cells given that variable jf_p%&gt;% group_by(memory_task_post)%&gt;% summarise(marginal_p = sum(p)) ## # A tibble: 5 x 2 ## memory_task_post marginal_p ## &lt;chr&gt; &lt;dbl&gt; ## 1 Adequate 0.192 ## 2 Excellent 0.225 ## 3 Good 0.408 ## 4 Poor 0.0917 ## 5 Terrible 0.0833 "]
]
