[
<<<<<<< HEAD
["index.html", "Psyc 6300: Data Management Chapter 1 Introduction", " Psyc 6300: Data Management James Rigby 2019-05-29 Chapter 1 Introduction "],
["prerequisites.html", "1.1 Prerequisites", " 1.1 Prerequisites This book assumes that you are familiar with the basics of the R language. Thus, we will not discuss basic arithmatic operators, common functions (i.e., mean), or data structures. Please review the material on base R if you are still uncomfortable with the foundations of the language. Datacamp offers a great set of courses (linked here) that will help get you up to speed. If you have yet to do so please install and load tidyverse by running the following code # Install tidyverse install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) "],
["supplemental-resources.html", "1.2 Supplemental Resources", " 1.2 Supplemental Resources This is by no means the only resource to learn data management skills in R. My aim is to provide a somewhat biased overview of how data management should be done in R. I draw heavily on packages from the tidyverse because they result in type consistent output and incorporate piping making them easier to use and interpret when compared to their base R counterparts. Here are additional that may provide different perspectives or additional insight into data management in R. Supplemental Resources Dplyr Cheatsheet Dplyr Vignette R For Data Scientists: Chapter 5 DataCamp: Data Manipulation with Dplyr Quick R: Data Management in Base R "],
["acknowledgements.html", "1.3 Acknowledgements", " 1.3 Acknowledgements This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. "],
||||||| merged common ancestors
["introduction.html", "Psyc 6300: Data Management Chapter 1 Introduction", " Psyc 6300: Data Management James Rigby 2019-05-28 Chapter 1 Introduction 1.0.1 Prerequisites This book assumes that you are familiar with the basics of the R language. Thus, we will not discuss basic arithmatic operators, common functions (i.e., mean), or data structures. Please review the material on base R if you are still uncomfortable with the foundations of the language. Datacamp offers a great set of courses (linked here) that will help get you up to speed. If you have yet to do so please install and load tidyverse by running the following code # Install tidyverse install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) 1.0.2 Supplemental Resources This is by no means the only resource to learn data management skills in R. My aim is to provide a somewhat biased overview of how data management should be done in R. I draw heavily on packages from the tidyverse because they result in type consistent output and incorporate piping making them easier to use and interpret when compared to their base R counterparts. Here are additional that may provide different perspectives or additional insight into data management in R. Supplemental Resources Dplyr Cheatsheet Dplyr Vignette R For Data Scientists: Chapter 5 DataCamp: Data Manipulation with Dplyr Quick R: Data Management in Base R 1.0.3 Acknowledgements This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. "],
=======
["index.html", "Psyc 6300: Data Management Chapter 1 Introduction", " Psyc 6300: Data Management James Rigby 2019-05-29 Chapter 1 Introduction 1.0.1 Prerequisites This book assumes that you are familiar with the basics of the R language. Thus, we will not discuss basic arithmatic operators, common functions (i.e., mean), or data structures. Please review the material on base R if you are still uncomfortable with the foundations of the language. Datacamp offers a great set of courses (linked here) that will help get you up to speed. If you have yet to do so please install and load tidyverse by running the following code # Install tidyverse install.packages(&quot;tidyverse&quot;) # Load tidyverse library(tidyverse) 1.0.2 Supplemental Resources This is by no means the only resource to learn data management skills in R. My aim is to provide a somewhat biased overview of how data management should be done in R. I draw heavily on packages from the tidyverse because they result in type consistent output and incorporate piping making them easier to use and interpret when compared to their base R counterparts. Here are additional that may provide different perspectives or additional insight into data management in R. Supplemental Resources Dplyr Cheatsheet Dplyr Vignette R For Data Scientists: Chapter 5 DataCamp: Data Manipulation with Dplyr Quick R: Data Management in Base R 1.0.3 Acknowledgements This class is supported by DataCamp, the most intuitive learning platform for data science. Learn R, Python and SQL the way you learn best through a combination of short expert videos and hands-on-the-keyboard exercises. Take over 100+ courses by expert instructors on topics such as importing data, data visualization or machine learning and learn faster through immediate and personalised feedback on every exercise. "],
>>>>>>> e1efedc12b12f886690e3712c3979efc5e3c2dc9
["material-overview.html", "Chapter 2 Material Overview", " Chapter 2 Material Overview If you are taking PSYC 6300 with me, this is the lecture plan for the classes covering data management. Day 1: Basic Dplyr Part 1: What is dplyr? (15 Minutes) Part 2: dplyr’s Core Functions (45 Minutes) Break (15 Minutes) Activity 1 (30 Minutes) Part 3: Bind and Join Functions (30 Minutes) Break (15 Minutes) Activity 2 (30 Minutes) Day 2: Advanced Dplyr and Tidyr Part 1: Functions for Extracting Observations Part 2: Key Helper Functions Break (15 Minutes) Activity 1 (30 Minutes) Part 3: Repeated Operations Part 4: Reshaping Data "],
["what-is-dplyr.html", "Chapter 3 What is dplyr?", " Chapter 3 What is dplyr? dplyr is a package that tries to provide a set of functions that utilizes a consistent design, philosophy, grammar, and data structure This consistency increases usability and interpretability of code It is consistently updated and supported by members of the R-Core team and creaters of RStudio It is the most commonly used to manipulate data within the R program "],
["why-is-data-manipulation-important.html", "3.1 Why is Data Manipulation Important?", " 3.1 Why is Data Manipulation Important? 3.1.1 Example 1: Survey Data 3.1.2 What’s Wrong With the Survey Data? Some of the meta-data collected by the survey platform is not meaningful. It is unclear what the data (i.e., Q1.1) is referring to. Items that start with Q1 and Q2 are associated with unique scales that need to be formed into composites. Some observations were created by you during pilot testing and should not be included. "],
["this-isnt-relevant-to-me-my-research-is-experimental.html", "3.2 This Isn’t Relevant to Me - My Research is Experimental!", " 3.2 This Isn’t Relevant to Me - My Research is Experimental! 3.2.1 Example 2: Experimental Data 3.2.2 What’s Wrong With the Experimental Data? Some of your participants figured out the purpose of your expiriment making their responses invalid. Your pre and post scale was miscalibrated and is .3 higher than it should be. Your pre and post measures are stored in seperate data files. Making matters more difficult, you have 17% participant attrition so you can’t just copy and paste data frames together. "],
["take-aways.html", "3.3 Take-Aways", " 3.3 Take-Aways Why Does Data Mangaement Matter? No matter what paradigm you work in data management is critical. Tidy data rarely exists in the wild. Efficiently managing data will save you time and make you a valuable assest to your collaborators. "],
["core-dplyr-functions.html", "Chapter 4 Core dplyr Functions", " Chapter 4 Core dplyr Functions Core dplyr Functions for Data Manipulation filter(): select rows based on some logical condition select(): select columns based on their names rename(): rename columns mutate(): add new variables that are functions of old - variables group_by(): perform grouped operations summarise(): Create summary statistics for a group arrange(): reorder rows based on some column "],
["form.html", "4.1 dplyr Function Structure", " 4.1 dplyr Function Structure All of the core dplyr functions take the following form: function(data, transformation, …) function: the dplyr function that you want to use data: the data frame or tibble you want to use the function on transformation: the transformation that you want to perform …: other transformations you want to perform "],
["filter.html", "4.2 filter(): Retaining Rows", " 4.2 filter(): Retaining Rows This function allows you to subset the data frame based on a logical test. Simply put, it allows you to choose which rows to keep. 4.2.1 filter() Structure filter(data, logical_test, …) Remember, all dplyr functions take the same general form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case transformation argument uses a logical test to define the observations we would like to keep. Logical tests can explicitly use logical operators (i.e., == or %in%). Functions that return logical values can also be used (i.e., is.na()). Multiple logical tests can be provided as indicated by the elipse. If tests are separated by a comma or ampersand, both tests must be TRUE for the observation to be retained. If tests are separated by a pipe (i.e., |), either argument can be satisfied for the observation to be retained 4.2.2 Using filter() Remember the survey data? Some observations were created when the survey was being tested. These observations not informative and should be removed. Luckily, the survey platform records whether a response is from a participant or a tester in the Status column (0 = participant, 8 = tester). Using filter(), we can easily retain the real observations while excluding rows associated with the pilot test. Example 4.1 Using filter to retain non-pilot observations (Status = 1). filter(ex_data, Status == 0) Example 4.2 A less practical example that retains observations that responded to Q1.1 OR Q1.3 with 5 filter(ex_data, Q1.1 == 5 | Q1.3 == 5 ) "],
["select-choosing-columns.html", "4.3 select(): Choosing Columns", " 4.3 select(): Choosing Columns Often when cleaning data, we only want to work a subset of columns. select() is used to retain or remove specific columns. 4.3.1 select() Structure select(data, cols_to_keep, …) Again, select() takes the general dplyr form (See section 4.1). The first argument specifies the data frame that we are manipulating. The second argument specifies the transformation we want to preform. In this case, the transformation argument specifies a column or columns we would like to keep, separated by commas. If you want to keep a range of columns you can specify the first column and last column of the range with a colon. Sometimes, it is more efficient to drop then select columns. To remove columns, simply include a minus sign in front of the column name. select() can also be used to reorder columns - the columns will be ordered how you type them. 4.3.2 Useful Helper Functions for select() starts_with() used in tandem select() allows you to keep variables that share a stem. ends_with() used in tandem with select() allows you to keep variables that share a suffix. contains() used in tandem with select() allows you to keep variables that share some common string anywhere in their structure. These can be used along with regular expressions to automate large portions of data cleaning. Helper Functions can speed up the data cleaning process while keeping your code easy to interpret. 4.3.3 Using select() Again, this function helps us solve two issues in the survey data example. The survey platform created a column of data for the participant’s last name that is completely empty. Furthermore, the Status column is no longer informative because all the values should equal 0. We can remove this column entirely using the select function. All of the following examples complete the same task using different methods although some are more efficient than others! Example 4.3 Using select() by specifying columns to retain. select(ex_data, ResponseId, Q1.1, Q1.2, Q1.3, Q2.1, Q2.2, Q2.3) Example 4.4 Using select() by specifying columns to omit. select(ex_data, -Status, -last_name) Example 4.5 Using select() by specifying range of columns. select(ex_data, ResponseId, Q1.1:Q2.3) Example 4.6 Using select() with helper functions. select(ex_data, contains(&quot;id&quot;, ignore.case = TRUE), starts_with(&quot;Q&quot;)) "],
["rename-renaming-variables.html", "4.4 rename(): Renaming Variables", " 4.4 rename(): Renaming Variables This function is very self explanatory - it renames columns (variables) 4.4.1 rename() Structure rename(data, old_name = new_name, …) Following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. In this case the transformation arguments take the form of an equation, where the new column name is on the left of the equals sign and the old column name is on the right. Multiple variables can be renamed within one rename call, as indicated by the elipse. 4.4.2 Using rename() Given that Q1.x and Q2.x are not meaningful stems, we should rename the items so that they are interpretable. It turns out that items that are labeled Q1 measured conscientiousness and items that are measured Q2 measure job performance. Example 4.7 Using rename() to provide substantive column names. rename(ex_data, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Example 4.8 select() can be used to rename columns as well! select(ex_data, ResponseId, cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3) Example 4.9 rename() may be one area where dplyr is lacking in efficiency. Here is the base R code to do the same task! colnames(ex_data)&lt;-c(&quot;ResponseId&quot;, paste0(&quot;cons&quot;, 1:3), paste0(&quot;perf&quot;, 1:3)) ex_data "],
["mutate-creating-new-variables.html", "4.5 mutate(): Creating New Variables", " 4.5 mutate(): Creating New Variables mutate() creates new variables that are defined by some function or operation. 4.5.1 mutate() Structure mutate(data, new_var = function, …) Again, following the general dplyr form (See section 4.1), the first argument specifies the data you are manipulating. The next argument specifies the transformation, which in mutate() defines a new variable. To do this, you specify a formula that specifies the name of a new variable on the left of the equals sign and a function that creates the new variable on the right. In this notation, function refers to any function or operator that creates a vector of output that is as long as the data frame or has a single value. Multiple new variables can be created within one mutate() call, but should be separated by commas. 4.5.2 Helper Functions rowwise(): Applies functions across columns within rows. ungroup(): Undoes grouping functions such as rowwise() and group_by() (group_by() will be discussed in Section 4.7) 4.5.3 Using mutate() Given that their are two sub scales (i.e., conscientiousness and performance) within our survey data, we can create scale scores for these sets of items. Typically this is done by averaging the item level data. mutate() provides an easy way to do this! Example 4.10 Using mutate() and arithmetic operators to create scale scores with missing data. mutate(ex_data, cons = (cons1+cons2+cons3)/3, perf = (perf1+perf2+perf3)/3) Example 4.11 Using mutate() and rowwise() to create scale scores while handling missing data (use with caution). ungroup(mutate(rowwise(ex_data), cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE))) "],
["the-pipe-operator.html", "4.6 The Pipe Operator (%&gt;%)", " 4.6 The Pipe Operator (%&gt;%) Notice that while cleaning the survey data we have been typing the data argument multiple times. Furthermore, using rowwise(), ungroup(), and mutate() all together makes our code difficult to read! Wouldn’t it be nice if there was some shorthand way to link functions together? Lucky for us there is, and it is call the pipe operator. The pipe operator carries forward the output of the previous function and uses it in the function that follows. This allows us to string together multiple functions without retyping the data argument. 4.6.1 Structure of the Pipe Operator function1(data, transformation, …)%&gt;% function2(transformation) The pipe operator carriers forward the output of the previous call and uses it in the subsequent function Thus, following any call with %&gt;% will carry forward the output into the subsequent function 4.6.2 Using the pipe operator Lets use the pipe operator to make our code more readable Example 4.12 Using pipe operator (%&gt;%) to redo what we have done thus far. ex_data%&gt;% filter(Status == 0)%&gt;% select(-Status, -last_name)%&gt;% rename(cons1 = Q1.1, cons2 = Q1.2, cons3 = Q1.3, perf1 = Q2.1, perf2 = Q2.2, perf3 = Q2.3)%&gt;% rowwise()%&gt;% mutate(cons = mean(c(cons1,cons2,cons3), na.rm = TRUE), perf = mean(c(perf1,perf2,perf3), na.rm = TRUE))%&gt;% ungroup() "],
["group.html", "4.7 group_by(): Grouping Data Frames", " 4.7 group_by(): Grouping Data Frames Sometimes, when working with data we want to perform some operation within a grouping variable. For example, the participants responding to this survey report to different managers. We may be interested in creating a new column of data that contains the work-groups’ average performance. group_by() can be used in tandem with mutate() to apply a function within columns clustering on groups 4.7.1 group_by Structure group_by(data, grouping_variable, …) group_by() takes the common dplyr structure - define the data and then define the transformation. The transformation in this case simply defines the grouping variable. If multiple grouping variables are provided, the data is grouped by unique combinations of all grouping variables. Note that this function is similar to rowwise() in that no physical change happens to the data - it only affects how later functions act the object. Because of this, group_by() is rarely (dare I say never) used without being accompanied by other functions such as mutate() or summarise() (to be covered in Section ??) Also, just like rowwise(), in order return the data set to its ungrouped form it is necessary to call the ungroup() function after finishing grouped manipulations. 4.7.2 Using group_by() The survey data has been joined with information regarding employees managers. We can now calculate each employee’s team’s average performance, conscientiousness, and the number of teammates who responded in the data. While I only illustrate how to use group_by() with the pipe operator, if for some reason you wanted to use a single group_by() call instead of a chain, it can be done. Example 4.13 Using group_by() to create team level variables and n() to create group size variables ex_data%&gt;% group_by(Manager)%&gt;% mutate(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(cons, na.rm = TRUE), team_size = n()) ## `mutate_if()` ignored the following grouping variables: ## Column `Manager` Example 4.14 add_count() is a nice alternative, to the group_by()%&gt;%mutate() chain if your goal is to simply add a grouped frequency variables to the data frame. ex_data%&gt;% add_count(Manager) ## summarise(): Creating Data Summaries {#sum} While creating grouped variables is sometimes necessary for analyses, often we simply want to describe properties of our data. summarise() is especially useful for this because it applies a function across rows of data to create a single value. If the data is grouped, there is a value returned for each group. 4.7.3 summarise() Structure summarise(data, summary_var = function, …) Following the consistent dplyr structure, summarise() requires that you first specify the data and then a transformation. The transformation in summarise takes a similar form as mutate(). The left hand side of the equation defines the name of a new summary variable and the right hand side defines a function or operation. The function should return a single value (i.e., mean() or sd()). 4.7.4 Using summarise() Let’s create a summary table for the overall sample as well as each team Example 4.15 Using summarise() to create a summary table for the entire survey data frame ex_data%&gt;% summarise(mean_cons = mean(cons, na.rm = TRUE), mean_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Example 4.16 Using group_by() and summarise() to create a summary table for different work groups ex_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE)) Example 4.17 count() is a nice alternative to the group_by()%&gt;%summarise() chain if your goal is simply to describe grouped frequencies. ex_data%&gt;% count(Manager) "],
["arrange-ordering-rows.html", "4.8 arrange(): Ordering Rows", " 4.8 arrange(): Ordering Rows arrange() can be used to sort rows in a data frame By default, arrange() orders a data frame from values in a column that go from smallest to largest You can use desc() with arrange to sort from largest to smallest 4.8.1 arrange() Structure arrange(data, sort_var, …) arrange() takes the same structure as all other core dplyr functions. First, specify the data you are manipulating Second, specify the transformation - the column or columns you are sorting by If multiple columns are provided, arrange will sort by the first column and use subsequent columns as tie breakers 4.8.2 Using arrange() Example 4.18 Adding arrange() to our summary table ex_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(team_cons) Example 4.19 Sorting the summary table in descending order with desc() and arrange() ex_data%&gt;% group_by(Manager)%&gt;% summarise(team_cons = mean(cons, na.rm = TRUE), team_perf = mean(perf, na.rm = TRUE), sd_cons = sd(cons, na.rm = TRUE), sd_perf = sd(perf, na.rm = TRUE))%&gt;% arrange(desc(team_cons)) "],
["activity-1.html", "Chapter 5 Activity 1", " Chapter 5 Activity 1 Now it’s your turn! You can find a data set titled “cons_perf.csv” in the supplemental material linked here. A .txt file (cons_perf_desc.txt) is also included describing the data and structure. Your goal is to clean the data and then summarise the data in a meaningful way. Instructions Keep observations that completed the survey AND were not from a test run. Remove columns that only contain missing data or a single value for every observation. Rename variables that start with Q to something meaningful. Create scale scores for the two different subscales in the data. Generate summaries for the two new subscales for the entire data set and a grouping variable Advanced Activity Did respondents complete the survey multiple times? On average how many times did each participant respond? Create scale scores that use mean(…, na.rm = TRUE) for participants that responded to 2 or more items, otherwise assign the respondent missing values. "],
["combining-data-sets.html", "Chapter 6 Combining Data Sets", " Chapter 6 Combining Data Sets Take a look the left panel of the tables above which shows the original survey data set after it was cleaned up. Manager information was not originally stored in this data! In order to get manager information into the data frame I did some magic behind the scenes. I merged the survey dataset the manager data in the right pane based on the ResponseId variable. Luckily the creators of dplyr wrote a set of functions that make merging multiple data tables easy. In the following sections we are going to learn a variety of different ways to bind and join data frames. "],
["binding-functions.html", "Chapter 7 Binding Functions", " Chapter 7 Binding Functions Binding functions are the most basic method used to combine data sets in the tidyverse, although they are not appropriate for all cases. In the sections that follow we will review what these funcitons do and highlight cases in which they are and are not appropriate. Binding Functions bind_rows(): Stacks many data frames vertically. bind_cols(): Joins many date frames horizontally. "],
["bind-cols-binding-data-frames-horizontally.html", "7.1 bind_cols(): Binding Data Frames Horizontally", " 7.1 bind_cols(): Binding Data Frames Horizontally bind_cols() is used when you have a set of data frames that Have equal number of rows Are ordered identically, with no missing or new observations If these two requirements are not met, the data will joined combining information about different observations or participants If your data does not meet either of these requirements, but has a participant identifier use a join function discussed below. 7.1.1 bind_cols() Structure bind_cols(…) bind_cols() takes two or more data frames (or a list of data frames) that have an equal number of identically ordered rows. 7.1.2 Using bind_cols() Note that, while for the survey data we want to combine two data frames horizontally, the data frames do not have the same number of rows. Furthemore, based on the responseId variable, we know that participants are not in the same order. Thus we are probably better off using a different function to join these two data sets. In contrast, note that in the two tabels below, each data frame has the same number of observations and the ID variables align perfectly. Example 7.1 Binding data frames together horizontally. bind_cols(cons_dat, perf_dat) Note that, since there is a dupicate column (ResponseId) in the new joined dataset, col_bind() automatically added a 1 to the end of the column name. This is intended to prevent mixups, but can result in dupicate data. While this example was adequate for illustration purposes, in practice, join functions are more flexible and appropriate when data sets have shared identifier. "],
["bind-rows-binding-data-frames-vertically.html", "7.2 bind_rows(): Binding Data Frames Vertically", " 7.2 bind_rows(): Binding Data Frames Vertically bind_rows() is used when you want to bind data frames vertically. This is sometimes referred to as stacking data frames. Unlike bind_cols(), bind_rows() attempts to match columns based on their names. If a data frame is missing a column, observations will have missing data for that variable. 7.2.1 bind_rows() Structure bind_rows(…) bind_rows() takes two or more data frames (or a list of data frames) 7.2.2 Using bind_rows() Example 7.2 Binding data frames together vertically (AKA, stacking). bind_rows(cons_dat, perf_dat) Note that the columns that are named the same are appropriately matched. Furthermore, observations from a data frames that is missing a column are assigned NA for that variable. "],
["mutating-joins.html", "Chapter 8 Mutating Joins", " Chapter 8 Mutating Joins Note that when we were using bind_cols(), corresponding rows in each data frame were assumed to belong to the same observation. This perfect matchup rarely occurs unless data frames were programatically spit and manipulated by the user. In psychological research, participant attrition causes some observations to be present in one data set but not another. Furthermore, participants can respond in a different orders across time-points. bind_cols() may also be inadequete when merging data frames associated with different levels in a hierarchy (i.e., team and individual). When merging heirarchical data frames, if one team is associated with many individuals, team information may need to be repeated multiple times. The mutating join functions were developed with these problems in mind. Mutating join functions share a number of common characteristics. They share the same structural form. Data frames are horizontally combined so that the outputted dataframe has more columns than either of the independent data frames. Rows are matched based on some common ID variable. If there are multiple rows with the same ID variable, all combinations of rows are returned. Despite their simularities, each mutating join differs in how it handles observations that do not match on an ID variable. In the sections that follow, I will: Define the common join fucntion form. Described how each join function handles observations that do not have a match on the ID variable. Provide examples of uses for each form. List of Mutating Joins left_join(): Joins based on an ID variable. Retains all rows in left data frame and only matching rows in the right. right_join(): Joins based on an ID variable. Retains all rows in the right data frame and only matching rows in the left. inner_join(): Joins based on an ID variable. Retains only matching rows for both data frames. full_join(): Joins based on an ID variable (Considers order). Retains only matching rows for "],
["joinform.html", "8.1 Join Functions: Structural Form", " 8.1 Join Functions: Structural Form function(x, y, by = c(“lh_id”, \"rh_id)) function denotes the type of join you would like to perform (i.e., full_join, left_join). Join functions commonly refer to left-hand and right-hand and data frames. Whether a data frame is a left-hand or right-hand data frame is determined by what order you enter your the two dataframes. x defines the left-hand data frame (it is the data frame argument furthest left). y defines the right-hand data frame (it is the data frame argument furtherst right). by is an optional argument that defines the ID variables that will be used to match rows lh_id is the ID variable in the left-hand data frame while rh_id is the ID in the right-hand data frame If by is left NULL, join functions will search the dataframes for columns that share names and join those. "],
["full-join.html", "8.2 full_join()", " 8.2 full_join() full_join() retains all rows from left- and right-hand data frames. Example 8.1 Joining the survey data with managerial data, retaining all observations from both data frames. full_join(cons_dat, man_data, by = c(&quot;ResponseId&quot;, &quot;ResponseId&quot;)) Example 8.2 Changing which data frame is the left-hand df and which data frame is the right-hand df changes the order of the columns but not which observations are kept. full_join(man_data, cons_dat, by = c(&quot;ResponseId&quot;, &quot;ResponseId&quot;)) Example 8.3 Since the data frames only share one variable with a commmon name. dplyr does give us a lovely message to let us know what the data frames are being joined by. This message will be suppressed from this point forward. full_join(man_data, cons_dat) ## Joining, by = &quot;ResponseId&quot; "],
["left.html", "8.3 left_join()", " 8.3 left_join() left_join() retains all observations in the left-hand data frame but only matching observations from the right-hand data frame. Example 8.4 Retaining all observations from the survey data, but only matching observations from the managerial data. left_join(cons_dat, man_data) Example 8.5 For left_join() order does matter. Different observations are retained depending on which data frame is in the left_hand position and which data frame is in the right-hand right hand position. left_join(man_data, cons_dat) "],
["right-join.html", "8.4 right_join()", " 8.4 right_join() Unsupprisingly, right_join() is the inverse of left_join(). It simply retains all observations in the right-hand data frame and only matching observations in the left-hand data frame Example 8.6 Note that this example produces output that is identical to Example 8.4. The only difference is the data frame arguments are flipped! right_join(man_data, cons_dat) "],
["inner-join.html", "8.5 inner_join()", " 8.5 inner_join() inner_join() only retains observations with matching IDs in both left-hand and right-hand data frames. For our example, every respondent has a manager making this function not very useful, right? That may be the case now that we have cleaned the data, but remember our original data recorded observations from our pilot tests. We removed these observations in Section 4.2. The graduate students pilot testing the survey would not have any record in the managerial data. Thus, inner_join() offers a different approach to excluding non-employees from our data along with non-respondents. Although the Status column is now gone, the first observation in the survey data below originated from a pilot test (Status == 8). Example 8.7 inner_join() will exclude all respondents for whom we do not have managerial information and all employees who did not respond to the survey. inner_join(man_data, unfiltered_cons_dat) "],
["filtering-joins.html", "Chapter 9 Filtering Joins", " Chapter 9 Filtering Joins While mutating joins are useful, sometimes it is necessary to remove observations from a data frame based on information stored elsewhere without adding any information to a focal data frame. Filtering joins do just that. As their title suggests, filtering joins are kind of a hybrid between filter() and the join family of functions. They take the same structural form as mutating joins (discussed in Section 8.1) but remove or retain observations that do not correspond to any observations ID variable in the rigth-hand data frame. Filtering Joins semi_join(): Retains rows in the left hand data frame that match an ID variable in the right hand data frame. anti_join(): Retains rows in the left hand data frame that do NOT match and ID variable in the right hand data frame. "],
["semi-join.html", "9.1 semi_join()", " 9.1 semi_join() semi_join() retains observations in the left-hand dataframe that have corresponding ID variables in the right-hand data frame. No new collumns are added to the left-hand dataframe. Example 9.1 Using semi_join() to retain observations for which we have managerial data without adding managerial data to the survey data. semi_join(unfiltered_cons_dat, man_data) "],
["anti-join.html", "9.2 anti_join()", " 9.2 anti_join() anti_join() retains observations in the left-hand data frame that do NOT have corresponding ID variables in the right-hand data frame. No new collumns are added to the left-hand dataframe. Example 9.2 Using anti_join() to identify employees in the managerial data frame that have not yet responded to our survey. anti_join( man_data, unfiltered_cons_dat) "],
["other-functions-for-extracting-observations.html", "Chapter 10 Other Functions for Extracting Observations", " Chapter 10 Other Functions for Extracting Observations In the previous chapters, we learned several functions that can be used to extract obseravtions from a data frame. filter() uses a logical test to extract observations. In contrast, filtering joins use an ID variable in another data frame to extract observations. In the chapters that follow we will cover functions that allow us to extract observations when we want to Want to take a random subset of observations Want to retain only distinct observations "],
["random-samples-of-observations.html", "10.1 Random Samples of Observations", " 10.1 Random Samples of Observations While very much beyond the scope of this lecture, randomly splitting a data set is a key operation for many statistical procedures. Researchers who want to cross-validated their models subset their data (sometimes \\(n\\) times) to evaluate its performance When a model’s statistical assumptions are violated, a researcher can implement a procedure called bootstrapping that uses resampling methods to estimate a parameters standard error. In short, knowing how to randomly sample a data set opens the door to many other statistical procedures The following sections will define the structural form, and show a few examples, however this section is not ended to go indepth into resampling methods. 10.1.1 Structural Form of sample_ Functions sample_x(data, size, replace, weight, …) sample_x denotes the sampling function you would like to use. data: specifies the data frame you would like to operate on. size: specifies the size of the sample you would like to take either in absolute or relative terms (depending on whether you use sample_n() or sample_frac()). replace: logical value specifying whether a data frame should be sampled with replacement (i.e., bootstrap). weight: a vector of weights equal to the number of observations in the dataframe specifying how likely each observation is to be sampled (useful for stratefied sampling). 10.1.2 Using sample_n() and sample_frac() sample_frac() and sample_n() only differ in terms of the size argument For sample_frac() you specify a proportion, relative to data argument. For sample_n() you specify an absolute number of rows for the output data. Arguably, sample_frac() is more robust to changes in upstream code. For example, if you catch a mistake in your data cleaning prior to taking a random sample of your data set, sample_frac() will still sample relative to this new data frame. In contrast, sample_n() does not adjust to changes in your code and will still resample based on the n you define. Putting differences aside, lets consider how a researcher could create a training and test data frame to evaluate their model’s performance. When randomly sampling the data frame, always make sure to use set.seed() so that the results are reproducible. I will also include and example that shows you how to work around the potential pitfalls fo sample_n() by avoiding hardcoding its size argument. Example 10.1 Using sample_n() and anti_join() to create training and test sets, hardcoding size. set.seed(123) training &lt;- sample_n(ex_data, size = 4, replace = FALSE) holdout &lt;- anti_join(ex_data, training) Example 10.2 Using sample_frac() and anti_join() to create training and test sets. set.seed(123) training &lt;- sample_frac(ex_data, size = .8, replace = FALSE) holdout &lt;- anti_join(ex_data, training) Example 10.3 Using sample_n() and anti_join() to create training and test sets, without hardcoding n. set.seed(123) rel_n&lt;- nrow(ex_data)*.8 training &lt;- sample_n(ex_data, size = rel_n, replace = FALSE) holdout &lt;- anti_join(ex_data, training) "],
["distinct-extracting-unique-observations.html", "10.2 distinct(): extracting unique observations", " 10.2 distinct(): extracting unique observations As we know, data collection methods are not perfect and neither are participants. Sometimes, software accidently records duplicate observations. Furthermore, participants may take surveys more than once resulting in duplicate information for the same person. Identifying distinct responses is often of critical step in ensuring high fidelity data. The final observation extraction function we will cover provides a means of extracting unique cases. 10.2.1 distinct() Structure distinct(data, distinct_var, …, .keep_all) data: specifies the data frame you would like to operate on. distinct_var: defines a variable that you would like to identify distinct levels of. If multiple variables are provided, distinct identifies unique combinations of these levels. .keep_all: is a logical values that specifies whether or not to keep all other variables in the resulting output. Note that distinct() returns the first observations with a distinct level of distinct_var. This means that if .keep_all = TRUE is only really appropriate when an observation is a true duplicate (all information is redundant). 10.2.2 Using distinct() Let’s take a look at the manager data that we used when combining multiple data frames. Let’s use distinct() create data frames that: Store the names of each manager. Store the IDs associated with each employee. Store ID/manager combinations. Example 10.4 Using distinct() to extract unique levels of manager. man_data%&gt;% distinct(Manager) Example 10.5 Using distinct() to extract unique levels of ResponseId. man_data%&gt;% distinct(Employee) Example 10.6 Using distinct() to extract unique ResponseId-Manager combinations. man_data%&gt;% distinct(Employee, Manager) man_data%&gt;% distinct(ResponseId, Manager)%&gt;% ggtexttable(theme = ttheme(base_style=&quot;lBlueWhite&quot;)) "],
["performing-repeated-operations.html", "Chapter 11 Performing Repeated Operations", " Chapter 11 Performing Repeated Operations By this point, my hope is that you feel comfortable with the core dplyr functions. In the next chapters, we will discuss three variations on each of these functions that allow you to write one line of code to repeat a manipulation many times. While I will not demonstrated every variation of these functions, knowing they exist can help you speed up your data cleaning. What are some instances when you would want to do repeated manipulations? Renaming all all columns so that their names are lower case. Centering a set of independent variables for regression. Calculating summary statistics for a large set of variables. Converting a set of variables that were read as character to numeric. Rounding all numeric variables to the second decimal place. Most of the core functions have variations that variations that facilitate repeated operations in different ways. The names are mostly the same, except a suffix is added to the end to differentiate it from its typical call (i.e., mutate_all()) Each suffix defines the repeated manipulation in a different way. Summary of Suffix Definitions _all: Applies the transformation to all columns. _at: Applies the transformation to a set of columns you define. _if: Applies the transformation to a set of columns that match an argument. "],
["all-suffix.html", "11.1 all() suffix", " 11.1 all() suffix As you might suspect, functions with the _all() suffix apply your specified transformation to all columns in the data frame. This is useful when there is a single transformation that is appropriate for every column. For example, because R is case sensitive, it is much easier to always were with lower case column names. Using rename_all() will help us rename every single column so that it matches this pattern. To use rename_all() we simply define the data frame and then a function that will take a column name and change it in some way. tolower() converts string values to lowercase and is the appropriate function to use here. Example 11.1 Using rename_all() and tolower() to rename all variables so they are lower case. ex_data%&gt;% rename_all(tolower) "],
["at-suffix.html", "11.2 at() suffix", " 11.2 at() suffix The at suffix applies a given transformation to a set of variables. It relies on the vars() helper function to define these variables. Let’s use the mutate_at() function to center the cons and perf columns In this case, I want to retain my centered and uncentered variables for later use. To do this, I defined a named list that contains the functions I want to apply to the variables I define. The names of the elements in the list will be appended to my original names to create new columns. .s are used as place holders for the vars. Example 11.2 Using mutate_at() to center cons and perf at 0. centered_data&lt;-ex_data%&gt;% mutate_at(vars(cons, perf), list(c = ~ .-mean(., na.rm = TRUE))) centered_data - Lets double check our work using summarise_at() - Centering a variables changes it’s expected value but not its spread. - This means that the new variables should have different means but identical standard deviations compared to the original variables. Example 11.3 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) "],
["if-suffix.html", "11.3 if() suffix", " 11.3 if() suffix The _if suffix applies a transformation to a set of columns that satisfy a logical test. I have been using mutate_if() and round() behind the scenes while writing this book to format most of the tables that you see. Consider Example 11.3 when I don’t use mutate_if(). Example 11.4 The code you saw really generates this ugly beast. centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE))) The floating point (maximum decimal point) in R goes out a long way making calculations very precise but output unwildy. By using mutate_if(), we can apply round() repeatedly across the data frame. round() only works with numeric data, though, so we want to avoid applying it character and factor data. Example 11.5 Using summarise_at() to verify the transformation worked appropriately centered_data%&gt;% summarise_at(vars(cons, perf, cons_c, perf_c), list(mean = ~mean(., na.rm = TRUE), sd = ~sd(., na.rm = TRUE)))%&gt;% mutate_if(is.numeric, round, digits = 5) "],
["multivariate-and-univariate-data.html", "Chapter 12 Multivariate and Univariate Data ", " Chapter 12 Multivariate and Univariate Data "],
["gather-wide-to-long.html", "12.1 gather(): Wide to Long", " 12.1 gather(): Wide to Long "],
["spread-long-to-wide.html", "12.2 spread(): Long to Wide", " 12.2 spread(): Long to Wide "],
["spliting-and-joining-columns.html", "Chapter 13 Spliting and Joining Columns ", " Chapter 13 Spliting and Joining Columns "],
["separate-splitting-columns.html", "13.1 separate(): Splitting Columns", " 13.1 separate(): Splitting Columns "],
["unite-joining-columns.html", "13.2 unite(): Joining Columns", " 13.2 unite(): Joining Columns "]
]
